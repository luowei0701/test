<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Publications">
    <meta name="author" content="WeiQM">
    <link rel="icon" href="images/logo/RMX_16.ico">
    <title>REMEX - Publications</title>
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="style/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!-- Custom styles for this template -->
    <link href="style/jquery.bxslider.css" rel="stylesheet">
    <link href="style/style.css" rel="stylesheet">

  </head>
  <body>
    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          </button>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li><a href="people.html">People</a></li>
            <li><a href="research.html">Research</a></li>
            <li class="active"><a href="publications.html">Publications</a></li>
            <li><a href="downloads.html">Downloads</a></li>
            <li><a href="contact.html">Contact Us</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li class="active"><a href="publications.html">English</a></li>
            <li><a href="html/cn/publications.html">中文</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="index.html"><img src="images/logo/logo_w.png" alt="Logo" width="80px"/></a></li>
          </ul>
        </div>
      </div>
    </nav>

    <div class="container">
    <header>
      <!--
      <a href="index.html"><img src="images/logo.png"  width="256px"></a>
      -->
    </header>

    <!--
    <section class="main-slider">
      <ul class="bxslider">
        <li><div class="slider-item"><img src="images/logo/logo_c.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_m.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_y.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_k.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_r.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_g.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_b.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
      </ul>
    </section>
    -->

    <section>
      <div class="row">
      <div class="col-md-12">
      <article class="content-block">
      <div class="block-body">
      <div class="block-text">

        <h3>2019</h3>
        <table><tbody>
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="./images/src/article_shi_MICCAI_2019.jpg" alt="Flowchart" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>Graph Convolutional Networks for Cervical Cell Classification</b> <a href="https://openreview.net/forum?id=S1gX_tlc-S" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                    Jun Shi, Ruoyu Wang, Yushan Zheng, Zhiguo Jiang, and Lanlan Yu
                  </i></font>
                  <br>
                   Medical Image Computing and Computer-Assisted Intervention, 2019
                  <br>
                  <i class="fa fa-file-pdf-o"></i> <a href="https://openreview.net/pdf?id=S1gX_tlc-S" target="_blank">PDF</a> &nbsp;
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('shiMICCAI2019Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('shiMICCAI2019Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="shiMICCAI2019Abs" class="abstract" style="display: none;">
                    Cervical cell classification is of important clinical significance in the screening of cervical cancer at early stages. In this paper, we present a novel cervical cell clas-sification method based on Graph Convolutional Network (GCN). In contrast with Convolutional Neural Networks (CNN) which can classify cervical cells through learned deep features, the proposed method uses GCN to explore the im-age-level potential relationship for improving the classification performance. Spe-cifically, each cervical cell image is represented by a pre-trained CNN. k-means clustering is performed on these CNN features and then the graph structure is constructed where each node is characterized by one cluster centroid. Conse-quently, the image-level relationship can be captured in terms of intrinsic cluster-ing structure. GCN is applied to propagate the underlying correlation of nodes and the relation-aware representation of GCN is incorporated to enrich the image-level CNN features. Experiments on the cervical cell image datasets demonstrate the effectiveness of our method.</p>
                <pre xml:space="preserve" id="shiMICCAI2019Bib" class="bibtex" style="display: none;">
  @article{shi2019graph,
    title={Graph Convolutional Networks for Cervical Cell Classification},
    author={Shi, Jun and Wang, Ruoyu and Zheng, Yushan and Jiang, Zhiguo and Yu, Lanlan},
    year={2019}
  }             
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('shiMICCAI2019Abs');
                  hideblock('shiMICCAI2019Bib');
                </script>
              </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="./images/src/article_zhang_sensors_2019.jpg" alt="Flowchart" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>A Comparable Study of CNN-Based Single Image Super-Resolution for Space-Based Imaging Sensors</b> <a href="https://www_mdpi.gg363.site/1424-8220/19/14/3234" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang, Pengrui Wang, Cong Zhang, and Zhiguo Jiang
                  </i></font>
                  <br>
                   Sensors, 2019
                  <br>
                  <i class="fa fa-file-pdf-o"></i> <a href="https://www_mdpi.gg363.site/1424-8220/19/14/3234/pdf" target="_blank">PDF</a> &nbsp;
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangsensors2019Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangsensors2019Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="zhangsensors2019Abs" class="abstract" style="display: none;">
                    In the case of space-based space surveillance (SBSS), images of the target space objects captured by space-based imaging sensors usually suffer from low spatial resolution due to the extremely long distance between the target and the imaging sensor. Image super-resolution is an effective data processing operation to get informative high resolution images. In this paper, we comparably study four recent popular models for single image super-resolution based on convolutional neural networks (CNNs) with the purpose of space applications. We specially fine-tune the super-resolution models designed for natural images using simulated images of space objects, and test the performance of different CNN-based models in different conditions that are mainly considered for SBSS. Experimental results show the advantages and drawbacks of these models, which could be helpful for the choice of proper CNN-based super-resolution method to deal with image data of space objects.</p>
                <pre xml:space="preserve" id="zhangsensors2019Bib" class="bibtex" style="display: none;">
  @article{zhang2019comparable,
    title={A Comparable Study of CNN-Based Single Image Super-Resolution for Space-Based Imaging Sensors},
    author={Zhang, Haopeng and Wang, Pengrui and Zhang, Cong and Jiang, Zhiguo},
    journal={Sensors},
    volume={19},
    number={14},
    pages={3234},
    year={2019},
    publisher={Multidisciplinary Digital Publishing Institute}
  }              
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('zhangsensors2019Abs');
                  hideblock('zhangsensors2019Bib');
                </script>
              </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="./images/src/article_zheng_miccai_2019.jpg" alt="Flowchart" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>Stain Standardization Capsule: A pre-processing module for histopathological image analysis</b> <a href="https://openreview.net/forum?id=B1xPG55qZS" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Yushan Zheng, Zhiguo Jiang, Haopeng Zhang, Jun Shi, and Fengying Xie
                  </i></font>
                  <br>
                    Medical Image Computing and Computer-Assisted Intervention, 2019
                  <br>
                  <i class="fa fa-file-pdf-o"></i> <a href="https://openreview.net/pdf?id=B1xPG55qZS" target="_blank">PDF</a> &nbsp;
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengMICCAI2019Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengMICCAI2019Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="zhengMICCAI2019Abs" class="abstract" style="display: none;">
                   Color consistency is crucial to developing robust deep learning methods for histopathological image analysis. With the increasing application of digital histopathological images, the deep learning methods are likely developed based on the data from multiple medical centers. This requirement makes it a challenge task to normalize the color variance of histopathological images from different medical centers. In this paper, we proposed a novel color standardization module named stain standardization capsule (SSC) based on the paradigm of capsule network and the corresponding dynamic routing algorithm. The proposed module can learn and generate uniform stain separation outputs for histopathological images in various color appearance without the reference to manually selected template images. The SSC module is light and can be trained end-to-end with the application-driven CNN model. The proposed method was validated on two public datasets and compared with the state-of-the-art methods. The experimental results have demonstrated that the SSC module is effective in color normalization for histopathological images and achieves the best performance in the compared methods.</p>
                <pre xml:space="preserve" id="zhengMICCAI2019Bib" class="bibtex" style="display: none;">
  @article{zheng2019stain,
    title={Stain Standardization Capsule: A pre-processing module for histopathological image analysis},
    author={Zheng, Yushan and Jiang, Zhiguo and Zhang, Haopeng and Shi, Jun and Xie, Fengying},
    year={2019}
  }
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('zhengMICCAI2019Abs');
                  hideblock('zhengMICCAI2019Bib');
                </script>
              </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="./images/src/article_zhang_IVC_2019.jpg" alt="Flowchart" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>Real-time 6D pose estimation from a single RGB image</b> <a href="https://www.sciencedirect.com/science/article/pii/S0262885619300964" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Xin Zhang, Zhiguo Jiang, and Haopeng Zhang
                  </i></font>
                  <br>
                    Image and Vision Computing, 2019
                  <br>
                  <i class="fa fa-file-pdf-o"></i> <a href="https://www.sciencedirect.com/science/article/pii/S0262885619300964/pdfft?md5=7539d5c7d17c768d5be862d483404b23&pid=1-s2.0-S0262885619300964-main.pdf" target="_blank">PDF</a> &nbsp;
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangIVC2019Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangIVC2019Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="zhangIVC2019Abs" class="abstract" style="display: none;">
                    We propose an end-to-end deep learning architecture for simultaneously detecting objects and recovering 6D poses in an RGB image. Concretely, we extend the 2D detection pipeline with a pose estimation module to indirectly regress the image coordinates of the object's 3D vertices based on 2D detection results. Then the object's 6D pose can be estimated using a Perspective-n-Point algorithm without any post-refinements. Moreover, we elaborately design a backbone structure to maintain spatial resolution of low level features for pose estimation task. Compared with state-of-the-art RGB based pose estimation methods, our approach achieves competitive or superior performance on two benchmark datasets at an inference speed of 25 fps on a GTX 1080Ti GPU, which is capable of real-time processing.</p>
                <pre xml:space="preserve" id="zhangIVC2019Bib" class="bibtex" style="display: none;">
  @article{zhang2019real,
    title={Real-time 6D pose estimation from a single RGB image},
    author={Zhang, Xin and Jiang, Zhiguo and Zhang, Haopeng},
    journal={Image and Vision Computing},
    volume={89},
    pages={1--11},
    year={2019},
    publisher={Elsevier}
  }
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('zhangIVC2019Abs');
                  hideblock('zhangIVC2019Bib');
                </script>
              </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="./images/src/article_zhang_IJAEV_2019.jpg" alt="Flowchart" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>Vision-Based Satellite Recognition and Pose Estimation Using Gaussian Process Regression</b> <a href="https://www.hindawi.com/journals/ijae/2019/5921246/abs/" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang, Cong Zhang, Zhiguo Jiang, Yuan Yao, and Gang Meng
                  </i></font>
                  <br>
                    International Journal of Aerospace Engineering Volume, 2019
                  <br>
                  <i class="fa fa-file-pdf-o"></i> <a href="http://downloads.hindawi.com/journals/ijae/2019/5921246.pdf" target="_blank">PDF</a> &nbsp;
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangIJAEV2019Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangIJAEV2019Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="zhangIJAEV2019Abs" class="abstract" style="display: none;">
                    In this paper, we address the problem of vision-based satellite recognition and pose estimation, which is to recognize the satellite from multiviews and estimate the relative poses using imaging sensors. We propose a vision-based method to solve these two problems using Gaussian process regression (GPR). Assuming that the regression function mapping from the image (or feature) of the target satellite to its category or pose follows a Gaussian process (GP) properly parameterized by a mean function and a covariance function, the predictive equations can be easily obtained by a maximum-likelihood approach when training data are given. These explicit formulations can not only offer the category or estimated pose by the mean value of the predicted output but also give its uncertainty by the variance which makes the predicted result convincing and applicable in practice. Besides, we also introduce a manifold constraint to the output of the GPR model to improve its performance for satellite pose estimation. Extensive experiments are performed on two simulated image datasets containing satellite images of 1D and 2D pose variations, as well as different noises and lighting conditions. Experimental results validate the effectiveness and robustness of our approach.</p>
                <pre xml:space="preserve" id="zhangIJAEV2019Bib" class="bibtex" style="display: none;">
  @article{zhang2019vision,
    title={Vision-Based Satellite Recognition and Pose Estimation Using Gaussian Process Regression},
    author={Zhang, Haopeng and Zhang, Cong and Jiang, Zhiguo and Yao, Yuan and Meng, Gang},
    journal={International Journal of Aerospace Engineering},
    volume={2019},
    year={2019},
    publisher={Hindawi}
  }
                    
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('zhangIJAEV2019Abs');
                  hideblock('zhangIJAEV2019Bib');
                </script>
              </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="./images/src/article_liu_RS_2019.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Unsupervised Saliency Model with Color Markov Chain for Oil Tank Detection</b> <a href="https://www_mdpi.gg363.site/2072-4292/11/9/1089" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Ziming Liu, Danpei Zhao, Zhenwei Shi, and Zhiguo Jiang
                </i></font>
                <br>
                  Remote Sensing, 2019
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="https://www_mdpi.gg363.site/2072-4292/11/9/1089/pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('liuRS2019Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('liuRS2019Bib')">BibTeX</a> &nbsp;
                &nbsp
              </p>
              <p id="liuRS2019Abs" class="abstract" style="display: none;">
                  Traditional oil tank detection methods often use geometric shape information. However, it is difficult to guarantee accurate detection under a variety of disturbance factors, especially various colors, scale differences, and the shadows caused by view angle and illumination. Therefore, we propose an unsupervised saliency model with Color Markov Chain (US-CMC) to deal with oil tank detection. To avoid the influence of shadows, we make use of the CIE Lab space to construct a Color Markov Chain and generate a bottom-up latent saliency map. Moreover, we build a circular feature map based on a radial symmetric circle, which makes true targets to be strengthened for a subjective detection task. Besides, we combine the latent saliency map with the circular feature map, which can effectively suppress other salient regions except for oil tanks. Extensive experimental results demonstrate that it outperforms 15 saliency models for remote sensing images (RSIs). Compared with conventional oil tank detection methods, US-CMC has achieved better results and is also more robust for view angle, shadow, and shape similarity problems.</p>
              <pre xml:space="preserve" id="liuRS2019Bib" class="bibtex" style="display: none;">
  @article{liu2019unsupervised,
    title={Unsupervised Saliency Model with Color Markov Chain for Oil Tank Detection},
    author={Liu, Ziming and Zhao, Danpei and Shi, Zhenwei and Jiang, Zhiguo},
    journal={Remote Sensing},
    volume={11},
    number={9},
    pages={1089},
    year={2019},
    publisher={Multidisciplinary Digital Publishing Institute}
  }
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('liuRS2019Abs');
                hideblock('liuRS2019Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="./images/src/article_zhang_IA_2019.png" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Star Detection and Accurate Centroiding for the Geosynchronous Interferometric Infrared Sounder of Fengyun-4A</b> <a href="https://ieeexplore_ieee.gg363.site/abstract/document/8629974/authors#authors" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Haopeng Zhang, Yi Su, Lei Yang, Jian Shang, Chengbao Liu, Jing Wang, and Shengxiong Zhou
                </i></font>
                <br>
                  IEEE Access, 2019
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="https://ieeexplore_ieee.gg363.site/stamp/stamp.jsp?tp=&arnumber=8629974" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangIA2019Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangIA2019Bib')">BibTeX</a> &nbsp;
                &nbsp
              </p>
              <p id="zhangIA2019Abs" class="abstract" style="display: none;">
                  Extracting accurate star centroids in the observed star images is one of the key problems for image navigation of the geosynchronous interferometric infrared sounder (GIIRS) of Fengyun-4A Satellite (FY-4A), the first scientific experimental satellite of the new generation of Chinese geostationary meteorological satellite Fengyun-4 series. Compared with star sensors which are widely used for star observation, it is challenging to detect the 2×2 star spot from the focused star images of GIIRS and calculate the star centroid in high precision. In this paper, we propose a star detection and centroiding method based on trajectory search and trajectory fitting. Since the launch of FY-4A in December 2016, our centroiding method has been tested in-orbit for over two years. The extensive experiments show that the star centroiding error of our method is less than 0.3 pixels, which makes an important contribution to image navigation of FY-4A.</p>
              <pre xml:space="preserve" id="zhangIA2019Bib" class="bibtex" style="display: none;">
  @article{zhang2019star,
    title={Star Detection and Accurate Centroiding for the Geosynchronous Interferometric Infrared Sounder of Fengyun-4A},
    author={Zhang, Haopeng and Su, Yi and Yang, Lei and Shang, Jian and Liu, Chengbao and Wang, Jing and Zhou, Shengxiong and Jiang, Zhiguo and Zhang, Zhiqing},
    journal={IEEE Access},
    volume={7},
    pages={18510--18520},
    year={2019},
    publisher={IEEE}
  }
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhangIA2019Abs');
                hideblock('zhangIA2019Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="./images/src/article_zhao_IEEE_2019.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Subjective Saliency Model Driven by Multi-Cues Stimulus for Airport Detection</b> <a href="https://ieeexplore_ieee.gg363.site/abstract/document/8653326" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Danpei Zhao, Jiayi Li, Zhenwei Shi, Zhiguo Jiang, and Cai Meng
                </i></font>
                <br>
                IEEE Access, 2019
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="https://ieeexplore_ieee.gg363.site/stamp/stamp.jsp?tp=&arnumber=8653326" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhaoIEEE2019Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhaoIEEE2019Bib')">BibTeX</a> &nbsp;
                &nbsp
              </p>
              <p id="zhaoIEEE2019Abs" class="abstract" style="display: none;">
                  Traditional saliency models designed for natural scene images usually use human visual characteristics to detect the target, but those salient areas in the remote sensing images (RSIs) may not be the targets we are really interested in. Taking both remote sensing image attributes and airport characteristics into account, we put forward a subjective saliency model driven by multi-cues stimulus for the airport's detection (MCS-SSM). Different from traditional saliency models, this model mainly relies on the subjective target detection task to find specific target area eliminating disturbance from other salient targets. Based on the low-level features, we train an LDA classifier by only small target samples and then build an object feature map. In the meantime, the shape information based on line density is extracted to get a shape map. Depending on the fusion result of two saliency maps, we optimize the subjective saliency map with SVM classifier. Moreover, MST density map is generated to suppress background and highlight interesting airport regions in the subjective saliency map. Consequently, MCS-SSM can respectively take the target, the background, and the detection task as multiple cues to quickly locate interest airport targets in RSI with a large cover area. MCS-SSM breaks through the limitations on color, texture, and other low-level characteristics compared with traditional saliency models, which are more targeted to detect the specific targets. The extensive experimental results demonstrate that the proposed MCS-SSM outperforms nine state-of-the-art saliency models. Besides, it has a higher detection rate and better effective performance than other three airport detection approaches.</p>
              <pre xml:space="preserve" id="zhaoIEEE2019Bib" class="bibtex" style="display: none;">
  @article{zhao2019subjective, 
    title={Subjective Saliency Model Driven by Multi-Cues Stimulus for Airport Detection}, 
    @article{zhao2019subjective,
      title={Subjective Saliency Model Driven by Multi-Cues Stimulus for Airport Detection},
      author={Zhao, Danpei and Li, Jiayi and Shi, Zhenwei and Jiang, Zhiguo and Meng, Cai},
      journal={IEEE Access},
      volume={7},
      pages={32118--32127},
      year={2019},
      publisher={IEEE}
    }
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhaoIEEE2019Abs');
                hideblock('zhaoIEEE2019Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="./images/src/article_yu_AST_2019.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Analytical entry guidance for coordinated flight with multiple no-fly-zone constraints</b> <a href="https://www.sciencedirect.com/science/article/pii/S1270963818311507" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Wenbin Yu， Wanchun Chen， Zhiguo Jiang， Wanqing Zhang， and Penglei Zhao
                </i></font>
                <br>
                  Aerospace Science and Technology, 2019
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="https://www.sciencedirect.com/science/article/pii/S1270963818311507/pdfft?md5=381d230161c8d0e983377961c5f9246d&pid=1-s2.0-S1270963818311507-main.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('yuAST2019Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('yuAST2019Bib')">BibTeX</a> &nbsp;
                &nbsp
              </p>
              <p id="yuAST2019Abs" class="abstract" style="display: none;">
                  This paper addresses the problem of coordinating a group of Hypersonic Glide Vehicles (HGVs) for the goal of simultaneous arrival in the presence of multiple No-Fly Zones (NFZs). Firstly, a high-precision analytical solution of flight time is derived from the nonlinear entry dynamics model built over a spherical and rotating Earth. Next, an entry guidance considering multi-NFZs and flight-time constraints is designed based on the new analytical flight-time formula as well as the existing 3-D analytical gliding-trajectory formulae. In the longitudinal part of the guidance, the flight-time and downrange formulae are used jointly to plan the longitudinal reference profile by considering both energy-management and flight-time requirements. In the lateral part, the downrange and crossrange formulae are used to plan the bank-reversal sequence according to the NFZ constraints. Additionally, in order to improve the accuracy of terminal time, speed, and altitude, a multi-objective iterative planning scheme employing onboard trajectory simulation is put forward and enabled at a time between the last two bank reversals to fine-tune the remaining short trajectory. In this scheme, the quasi-Newton method is improved by the use of directional derivatives such that the number of the trajectory simulations required to calculate the Jacobian matrix is reduced from 3 to 2 in each iteration, which greatly reduced the amount of calculation. Finally, a flight-time coordination scheme is developed for multiple HGVs to determine the starting times of entry flight, which can further determine the launch times once the boost guidance is specified. The superior performance of the guidance is demonstrated by Monte-Carlo simulations in stochastic disturbed circumstances.</p>
              <pre xml:space="preserve" id="yuAST2019Bib" class="bibtex" style="display: none;">
  @article{yu2019analytical,
    title={Analytical entry guidance for coordinated flight with multiple no-fly-zone constraints},
    author={Yu, Wenbin and Chen, Wanchun and Jiang, Zhiguo and Zhang, Wanqing and Zhao, Penglei},
    journal={Aerospace Science and Technology},
    volume={84},
    pages={273--290},
    year={2019},
    publisher={Elsevier}
  }
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('yuAST2019Abs');
                hideblock('yuAST2019Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="./images/src/article_zheng_cmpb_2019.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Adaptive Color Deconvolution For Histological WSI Normalization</b> <a href="https://www.sciencedirect.com/science/article/pii/S0169260718312161" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Yushan Zheng, Zhiguo Jiang, Haopeng Zhang, Fengying Xie, Jun Shi, and Chenghai Xue
                </i></font>
                <br>
                  Computer Methods and Programs in Biomedicine, 2019
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_cmpb_2019.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengCMPB2019Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengCMPB2019Bib')">BibTeX</a> &nbsp;
                <i class="fa fa-code"></i> <a href="https://github.com/Zhengyushan/adaptive_color_deconvolution" target="_blank">Code</a> &nbsp
              </p>
              <p id="zhengCMPB2019Abs" class="abstract" style="display: none;">
                <b>Background and Objective</b>
                Color consistency of histological images is significant for developing reliable computer-aided diagnosis (CAD) systems. However, the color appearance of digital histological images varies across different specimen preparations, staining, and scanning situations. This variability affects the diagnosis and decreases the accuracy of CAD approaches. It is important and challenging to develop effective color normalization methods for digital histological images.
                <b>Methods</b>
                We proposed a novel adaptive color deconvolution (ACD) algorithm for stain separation and color normalization of hematoxylin-eosin-stained whole slide images (WSIs). To avoid artifacts and reduce the failure rate of normalization, multiple prior knowledges of staining are considered and embedded in the ACD model. To improve the capacity of color normalization for various WSIs, an integrated optimization is designed to simultaneously estimate the parameters of the stain separation and color normalization. The solving of ACD model and application of the proposed method involves only pixel-wise operation, which makes it very efficient and applicable to WSIs.
                <b>Results</b>
                The proposed method was evaluated on four WSI-datasets including breast, lung and cervix cancers and was compared with 6 state-of-the-art methods. The proposed method achieved the most consistent performance in color normalization according to the quantitative metrics. Through a qualitative assessment for 500 WSIs, the failure rate of normalization was 0.4% and the structure and color artifacts were effectively avoided. Applied to CAD methods, the area under receiver operating characteristic curve for cancer image classification was improved from 0.842 to 0.914. The average time of solving the ACD model is 2.97 s.
                <b>Conclusions</b>
                The proposed ACD model has prone effective for color normalization of hematoxylin-eosin-stained WSIs in various color appearances. The model is robust and can be applied to WSIs containing different lesions. The proposed model can be efficiently solved and is effective to improve the performance of cancer image recognition, which is adequate for developing automatic CAD programs and systems based on WSIs.</p>
              <pre xml:space="preserve" id="zhengCMPB2019Bib" class="bibtex" style="display: none;">
@article{zhengCMPB2019,
  title   = {Adaptive color deconvolution for histological WSI normalization},
  author  = {Yushan Zheng and Zhiguo Jiang and Haopeng Zhang and Fengying Xie and Jun Shi and Chenghai Xue},
  journal = {Computer Methods and Programs in Biomedicine},
  volume  = {170},
  pages   = {107-120},
  doi     = {doi.org/10.1016/j.cmpb.2019.01.008},
  year    = {2019}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhengCMPB2019Abs');
                hideblock('zhengCMPB2019Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="./images/src/airticle_yao_RS_2019.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>On-Board Ship Detection in Micro-Nano Satellite Based on Deep Learning and COTS Component</b> <a href="https://www.mdpi.com/2072-4292/11/7/762" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Yuan Yao, Zhiguo Jiang, Haopeng Zhang, and Yu Zhou
                </i></font>
                <br>
                  Remote Sensing, 2019<br>
                  Dataset : <a href="https://pan.baidu.com/s/16of1rN6U1dVB5DcCe0ytBQ">   https://pan.baidu.com/s/16of1rN6U1dVB5DcCe0ytBQ passwd:uj3n
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="./source/pdf/article_yao_rs_2019.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('yaoRS2019Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('yaoRS2019Bib')">BibTeX</a> &nbsp;
                &nbsp
              </p>
              <p id="yaoRS2019Abs" class="abstract" style="display: none;">
                Micro-nano satellites have provided a large amount of remote sensing images for many earth observation applications. However, the hysteresis of satellite-ground mutual communication of massive remote sensing images and the low efficiency of traditional information processing flow have become the bottlenecks for the further development of micro-nano satellites. To solve this problem, this paper proposes an on-board ship detection scheme based on deep learning and Commercial Off-The-Shelf (COTS) component, which can be used to achieve near real-time on-board processing by micro-nano satellite computing platform. The on-board ship detection algorithm based on deep learning consists of a feature extraction network, Region Proposal Network (RPN) with square anchors, Global Average Pooling (GAP), and Bigger-Left Non-Maximum Suppression (BL-NMS). With the help of high performance COTS components, the proposed scheme can extract target patches and valuable information from remote sensing images quickly and accurately. A ground demonstration and verification system is built to verify the feasibility and effectiveness of our scheme. Our method achieves the performance with 95.9% recall and 80.5% precision in our dataset. Experimental results show that the scheme has a good application prospect in micro-nano satellites with limited power and computing resources.</p>
              <pre xml:space="preserve" id="yaoRS2019Bib" class="bibtex" style="display: none;">
@article{yaoRS2019,
  title   = {On-Board Ship Detection in Micro-Nano Satellite Based on Deep Learning and COTS Component},
  author  = {Yuan Yao and Zhiguo Jiang and Haopeng Zhang and Yu Zhou},
  journal = {Remote Sensing},
  volume  = {11},
  doi     = {10.3390/rs11070762},
  year    = {2019}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('yaoRS2019Abs');
                hideblock('yaoRS2019Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
        </tbody></table>

        <hr />
        <!-- -------------------------------------------- -->
        <!-- -------------------------------------------- -->
        <h3>2018</h3>
        <table><tbody>
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="./images/src/article_cai_RS_2018.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Online Exemplar-Based Fully Convolutional Network for Aircraft Detection in Remote Sensing Images</b> <a href="https://ieeexplore.ieee.org/document/8356628/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Bowen Cai, Zhiguo Jiang, Haopeng Zhang, Yuan Yao and Shanlan Nie
                </i></font>
                <br>
                  IEEE GEOSCIENCE AND REMOTE SENSING LETTERS, 2018
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="./source/pdf/article_cai_RS_2018.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('caiRS2018Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('caiRS2018Bib')">BibTeX</a> &nbsp;
                &nbsp
              </p>
              <p id="caiRS2018Abs" class="abstract" style="display: none;">
                Convolutional neural network obtains remarkable achievements on target detection, due to its prominent capability on feature extraction. However, it still needs further study for aircraft detection task, since intraclass variation still restricts the accuracy of aircraft detection in remote sensing images. In this letter, we adopt regularity of aircraft circle response to design our end-to-end fully convolutional network (FCN), and embed online exemplar mining into our network to handle intraclass variation. The mined exemplars are employed to capture different intraclass characteristics, which effectively reduces the burden of network training. Specifically, we first select basic exemplars based on labeled information and initialize the relationships between exemplars and aircraft examples. Then, these relationships will be updated by the similarity of these examples in high-level features space. Finally, aircraft examples will be used to train different exemplar detectors according to updated relationships. Motivated by the geometric shape of aircraft, a circle response map is developed to construct our FCN to achieve more efficient aircraft detection. The comparative experiments indicate that superior performance of our network in accurate and efficient aircraft detection..</p>
              <pre xml:space="preserve" id="caiRS2018Bib" class="bibtex" style="display: none;">
@article{caiRS2018,
  title   = {Online Exemplar-Based Fully Convolutional Network for Aircraft Detection in Remote Sensing Images},
  author  = {Bowen Cai and Zhiguo Jiang and Haopeng Zhang and Yuan Yao and Shanlan Nie},
  journal = {IEEE GEOSCIENCE AND REMOTE SENSING LETTERS},
  volume  = {15},
  pages   = {1095-1099},
  doi     = {10.1109/LGRS.2018.2829147},
  year    = {2018}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('caiRS2018Abs');
                hideblock('caiRS2018Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_yang_ijgi_2018.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Higher Order Support Vector Random Fields for Hyperspectral Image Classification</b> <a href="http://apps.webofknowledge.com/full_record.do?product=WOS&search_mode=GeneralSearch&qid=8&SID=6Dt9E3Uj6j9E5kXWUJC&page=1&doc=1" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Junli Yang, Zhiguo Jiang, Shuang Hao and Haopeng Zhang
                </i></font>
                <br>
                ISPRS International Journal of Geo-Information, 2018
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('yangIJGI2018Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('yangIJGI2018Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="yangIJGI2018Abs" class="abstract" style="display: none;">
                  This paper addresses the problem of contextual hyperspectral image (HSI) classification. A novel conditional random fields (CRFs) model, known as higher order support vector random fields (HSVRFs), is proposed for HSI classification. By incorporating higher order potentials into a support vector random fields with a Mahalanobis distance boundary constraint (SVRFMC) model, the HSVRFs model not only takes advantage of the support vector machine (SVM) classifier and the Mahalanobis distance boundary constraint, but can also capture higher level contextual information to depict complicated details in HSI. The higher order potentials are defined on image segments, which are created by a fast unsupervised over-segmentation algorithm. The higher order potentials consider the spectral vectors of each of the segment's constituting pixels coherently, and weight these pixels with the output probability of the support vector machine (SVM) classifier in our framework. Therefore, the higher order potentials can model higher-level contextual information, which is useful for the description of challenging complex structures and boundaries in HSI. Experimental results on two publicly available HSI datasets show that the HSVRFs model outperforms traditional and state-of-the art methods in HSI classification, especially for datasets containing complicated details.
              </p>
              <pre xml:space="preserve" id="yangIJGI2018Bib" class="bibtex" style="display: none;">
@article{yangIJGI2018,
  title   = {Higher Order Support Vector Random Fields for Hyperspectral Image Classification},
  author  = {Junli Yang and Zhiguo Jiang and Shuang Hao and Haopeng Zhang},
  journal = {2018 ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION(IJGI)},
  volume  = {7},
  number  = {1},
  pages   = {19},
  doi     = {10.3390/ijgi7010019},
  year    = {2018}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('yangIJGI2018Abs');
                hideblock('yangIJGI2018Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_ma_jbhi_2016.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Generating Region Proposals for Histopathological Whole Slide Image Retrieval</b> <a href="http://www.sciencedirect.com/science/article/pii/S0169260717312154" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Yibing Ma, Zhiguo Jiang, Haopeng Zhang, Fengying Xie, Yushan Zheng, Huaqiang Shi, Yu Zhao and Jun Shi
                </i></font>
                <br>
                  Computer Methods and Programs in Biomedicine, 2018
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('maCMBP2018Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('maCMBP2018Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="maCMBP2018Abs" class="abstract" style="display: none;">
                  <b>Background and objective</b>
                  Content-based image retrieval is an effective method for histopathological image analysis. However, given a database of huge whole slide images (WSIs), acquiring appropriate region-of-interests (ROIs) for training is significant and difficult. Moreover, histopathological images can only be annotated by pathologists, resulting in the lack of labeling information. Therefore, it is an important and challenging task to generate ROIs from WSI and retrieve image with few labels.
                  <b>Methods</b>
                  This paper presents a novel unsupervised region proposing method for histopathological WSI based on Selective Search. Specifically, the WSI is over-segmented into regions which are hierarchically merged until the WSI becomes a single region. Nucleus-oriented similarity measures for region mergence and Nucleus–Cytoplasm color space for histopathological image are specially defined to generate accurate region proposals. Additionally, we propose a new semi-supervised hashing method for image retrieval. The semantic features of images are extracted with Latent Dirichlet Allocation and transformed into binary hashing codes with Supervised Hashing.
                  <b>Results</b>
                  The methods are tested on a large-scale multi-class database of breast histopathological WSIs. The results demonstrate that for one WSI, our region proposing method can generate 7.3 thousand contoured regions which fit well with 95.8% of the ROIs annotated by pathologists. The proposed hashing method can retrieve a query image among 136 thousand images in 0.29 s and reach precision of 91% with only 10% of images labeled.
                  <b>Conclusions</b>
                  The unsupervised region proposing method can generate regions as predictions of lesions in histopathological WSI. The region proposals can also serve as the training samples to train machine-learning models for image retrieval. The proposed hashing method can achieve fast and precise image retrieval with small amount of labels. Furthermore, the proposed methods can be potentially applied in online computer-aided-diagnosis systems.
                </p>
              <pre xml:space="preserve" id="maCMBP2018Bib" class="bibtex" style="display: none;">
@article{maCMPB2018,
  title   = {Generating region proposals for histopathological whole slide image retrieval},
  author  = {Yibing Ma and Zhiguo Jiang and Haopeng Zhang and Fengying Xie 
            and Yushan Zheng and Huaqiang Shi and Yu Zhao and Jun Shi},
  journal = {Computer Methods and Programs in Biomedicine},
  volume  = {159},
  pages   = {1 - 10},
  year    = {2018},
  issn    = {0169-2607},
  url     = {http://www.sciencedirect.com/science/article/pii/S0169260717312154},
}      
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('maCMBP2018Abs');
                hideblock('maCMBP2018Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zheng_tmi_2018.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Histopathological Whole Slide Image Analysis Using Context-based CBIR</b> <a href="http://ieeexplore.ieee.org/document/8265156/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yushan Zheng, Zhiguo Jiang, Haopeng Zhang, Fengying Xie, Yibing Ma, Huaqiang Shi and Yu Zhao
                </i></font>
                <br>
                 IEEE Transactions on Medical Imaging, 2018
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_tmi_2018.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengTMI2018Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengTMI2018Bib')">BibTeX</a> &nbsp;
                <i class="fa fa-link"></i> <a href="source/pdf/article_zheng_tmi_2018_sup.pdf" target="_blank">Supplementary</a> &nbsp;
              </p>
              <p id="zhengTMI2018Abs" class="abstract" style="display: none;">
                Histopathological image classification (HIC) and content-based histopathological image retrieval (CBHIR) are two promising applications for histopathological whole slide image (WSI) analysis. HIC can efficiently predict the type of lesion involved in a histopathological image. In general, HIC can aid pathologists in locating high-risk cancer regions from a WSI by providing a cancerous probability map for the WSI. In contrast, CBHIR was developed to allow searches for regions with similar content for a region of interest (ROI) from a database consisting of historical cases. Sets of cases with similar content are accessible to pathologists, which can provide more valuable references for diagnosis. A drawback of the recent CBHIR framework is that a query ROI needs to be manually selected from a WSI. An automatic CBHIR approach for a WSI-wise analysis needs to be developed. In this paper, we propose a novel aided-diagnosis framework of breast cancer using whole slide images, which shares the advantages of both HIC and CBHIR. In our framework, CBHIR is automatically processed throughout the WSI, based on which a probability map regarding the malignancy of breast tumors is calculated. Through the probability map, the malignant regions in WSIs can be easily recognized. Furthermore, the retrieval results corresponding to each sub-region of the WSIs are recorded during the automatic analysis and are available to pathologists during their diagnosis. Our method was validated on fully annotated WSI datasets of breast tumors. The experimental results certify the effectiveness of the proposed method.
              </p>
              <pre xml:space="preserve" id="zhengTMI2018Bib" class="bibtex" style="display: none;">
@article{zhengTMI18,
  author  = {Yushan Zheng and Zhiguo Jiang and Haopeng Zhang and Fengying Xie
             and Yibing Ma and Huaqiang Shi and Yu Zhao},
  title   = {Histopathological Whole Slide Image Analysis Using Context-based CBIR},
  journal = {IEEE Transactions on Medical Imaging},
  doi     = {10.1109/TMI.2018.2796130},
  year    = {Epub 2018 January 23}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhengTMI2018Abs');
                hideblock('zhengTMI2018Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_wei_sensors_2018.jpg" alt="Results of article_wei_sensors_18" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Robust Spacecraft Component Detection in Point Clouds</b> <a href="http://www.mdpi.com/1424-8220/18/4/933" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  <a href="https://weiquanmao.github.io" target="_blank">Quanmao Wei</a>, Zhiguo Jiang and Haopeng Zhang
                </i></font>
                <br>
                Sensors, 2018
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="http://www.mdpi.com/1424-8220/18/4/933/pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('weiSensors18Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('weiSensors18Bib')">BibTeX</a> &nbsp;
                <i class="fa fa-link"></i> <a href="http://www.mdpi.com/1424-8220/18/4/933/s1" target="_blank">Supplementary</a> &nbsp;
                <i class="fa fa-code"></i> <a href="https://github.com/weiquanmao/PCF" target="_blank">Code</a> &nbsp
              </p>
              <p id="weiSensors18Abs" class="abstract" style="display: none;">
                Automatic component detection of spacecraft can assist in on-orbit operation and space situational awareness. Spacecraft are generally composed of solar panels and cuboidal or cylindrical modules. These components can be simply represented by geometric primitives like plane, cuboid and cylinder. Based on this prior, we propose a robust automatic detection scheme to automatically detect such basic components of spacecraft in three-dimensional (3D) point clouds. In the proposed scheme, cylinders are first detected in the iteration of the energy-based geometric model fitting and cylinder parameter estimation. Then, planes are detected by Hough transform and further described as bounded patches with their minimum bounding rectangles. Finally, the cuboids are detected with pair-wise geometry relations from the detected patches. After successive detection of cylinders, planar patches and cuboids, a mid-level geometry representation of the spacecraft can be delivered. We tested the proposed component detection scheme on spacecraft 3D point clouds synthesized by computer-aided design (CAD) models and those recovered by image-based reconstruction, respectively. Experimental results illustrate that the proposed scheme can detect the basic geometric components effectively and has fine robustness against noise and point distribution density.
              </p>
              <pre xml:space="preserve" id="weiSensors18Bib" class="bibtex" style="display: none;">
@article{weiSensors18,
  author  = {Quanmao Wei and Zhiguo Jiang and Haopeng Zhang},
  title   = {Robust Spacecraft Component Detection in Point Clouds},
  journal = {Sensors},
  volume  = {18},
  year    = {2018},
  number  = {4},
  article number = {933},
  url     = {http://www.mdpi.com/1424-8220/18/4/933},
  issn    = {1424—8220},
  doi     = {10.3390/s18040933}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('weiSensors18Abs');
                hideblock('weiSensors18Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zhang_taes_2018.jpg" alt="Results of article_zhang_taes_18" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Vision-based Pose Estimation for Textureless Space Objects by Contour Points Matching</b> <a href="http://ieeexplore.ieee.org/document/8315479/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Xin Zhang, Zhiguo Jiang, Haopeng Zhang and <a href="https://weiquanmao.github.io" target="_blank">Quanmao Wei</a>
                </i></font>
                <br>
                IEEE Transactions on Aerospace and Electronic Systems, 2018
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8315479" target="_blank">Preprint</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangTAES18Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangTAES18Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhangTAES18Abs" class="abstract" style="display: none;">
                This paper presents a novel vision-based method to solve the 6-degree-of-freedom pose estimation problem of textureless space objects from a single monocular image. Our approach follows a coarse-to-fine procedure, utilizing only shape and contour information of the input image. To achieve invariance to initialization, we select a series of projection images which are similar to the input image and establish many-to-one 2D-3D correspondences by contour feature matching. Intensive attention is focused on outlier rejection and we introduce an innovative strategy to fully utilize geometric matching information to guide pose calculation. Experiments based on simulated images are carried out, and the results manifest that pose estimation error of our approach is about 1% even in situations with heavy outlier correspondences.
              </p>
              <pre xml:space="preserve" id="zhangTAES18Bib" class="bibtex" style="display: none;">
@article{zhangTAES18,
  author  = {Xin Zhang and Zhiguo Jiang and Haopeng Zhang and Quanmao Wei},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  title   = {Vision-based Pose Estimation for Textureless Space Objects
             by Contour Points Matching},
  year    = {2018},
  month   = {},
  volume  = {PP},
  number  = {99},
  pages   = {1—-1},
  issn    = {0018-9251},
  doi     = {10.1109/TAES.2018.2815879}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhangTAES18Abs');
                hideblock('zhangTAES18Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
        </tbody></table>

        <hr />
        <!-- -------------------------------------------- -->
        <!-- -------------------------------------------- -->
        <h3>2017</h3>
        <table><tbody>
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zheng_jbhi_2017.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Size-scalable Content-based Histopathological Image Retrieval from Ratabase that Ronsists of WSIs</b> <a href="http://ieeexplore.ieee.org/document/7967806/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yushan Zheng, Zhiguo Jiang, Haopeng Zhang, Fengying Xie, Yibing Ma, Huaqiang Shi and Yu Zhao
                </i></font>
                <br>
                IEEE Journal of Biomedical and Health Informatics, 2017
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_jbhi_2017.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengJBHI2017Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengJBHI2017Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhengJBHI2017Abs" class="abstract" style="display: none;">
                Content-based image retrieval (CBIR) has been widely researched for histopathological images. It is challenging to retrieve contently similar regions from histopathological whole slide images (WSIs) for regions of interest (ROIs) in different size. In this paper, we propose a novel CBIR framework for database that consists of WSIs and size-scalable query ROIs. Each WSI in the database is encoded into a matrix of binary codes. When retrieving, a group of region proposals that have similar size with the query ROI are firstly located in the database through an efficient table-lookup approach. Then, these regions are ranked by a designed multi-binary-code-based similarity measurement. Finally, the top relevant regions and their locations in the WSIs as well as the corresponding diagnostic information are returned to assist pathologists. The effectiveness of the proposed framework is evaluated on a fine-annotated WSI database of epithelial breast tumors. The experimental results have proved that the proposed framework is effective for retrieval from database that consists of WSIs. Specifically, for query ROIs of 4096$\times$4096 pixels, the retrieval precision of the top 20 return has reached 96\% and the retrieval time is less than 1.5 second.
              </p>
              <pre xml:space="preserve" id="zhengJBHI2017Bib" class="bibtex" style="display: none;">
@article{zhengJBHI17,
  author  = {Yushan Zheng and Zhiguo Jiang Haopeng Zhang and Fengying Xie
             and Yibing Ma and Huaqiang Shi and Yu Zhao},
  title   = {Size-scalable Content-based Histopathological Image Retrieval
             from Database that Consists of WSIs},
  journal = {IEEE journal of biomedical and health informatics},
  doi     = {10.1109/jbhi.2017.2723014},
  year    = {2017}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhengJBHI2017Abs');
                hideblock('zhengJBHI2017Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zheng_pr_2017.jpg" alt="Flowchart_of_N_CNN" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Feature Extraction from Histopathological Images Based on Nucleus-guided Convolutional Neural Network for Breast Lesion Classification</b> <a href="https://www.sciencedirect.com/science/article/pii/S0031320317302005?via%3Dihub" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yushan Zheng, Zhiguo Jiang, Fengying Xie, Haopeng Zhang, Yibing Ma, Huaqiang Shi and Yu Zhao
                </i></font>
                <br>
                Pattern Recognition, 2017
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_pr_2017.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengPR2017Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengPR2017Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhengPR2017Abs" class="abstract" style="display: none;">
                Feature extraction is a crucial and challenging aspect in the computer-aided diagnosis of breast cancer with histopathological images. In recent years, many machine learning methods have been introduced to extract features from histopathological images. In this study, a novel nucleus-guided feature extraction framework based on convolutional neural network is proposed for histopathological images. The nuclei are first detected from images, and then used to train a designed convolutional neural network with three hierarchy structures. Through the trained network, image-level features including the pattern and spatial distribution of the nuclei are extracted. The proposed features are evaluated through the classification experiment on a histopathological image database of breast lesions. The experimental results show that the extracted features effectively represent histopathological images, and the proposed framework achieves a better classification performance for breast lesions than the compared state-of-the-art methods.
              </p>
              <pre xml:space="preserve" id="zhengPR2017Bib" class="bibtex" style="display: none;">
@article{zhengPR17,
  author  = {Yushan Zheng and Zhiguo Jiang and Fengying Xie and Haopeng Zhang
             and Yibing Ma and Huaqiang Shi and Yu Zhao},
  title   = {Feature Extraction from Histopathological Images Based on Nucleus-guided
             Convolutional Neural Network for Breast Lesion Classification},
  journal = {Pattern Recognition},
  year    = {2017},
  volume  = {71},
  pages   = {14—-25},
  issn    = {0031-3203},
  doi     = {10.1016/j.patcog.2017.05.010}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhengPR2017Abs');
                hideblock('zhengPR2017Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zhang_sensors_2017.jpg" alt="Results of article_zhang_sensors_17" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>3D Reconstruction of Space Objects from Multi-Views by A Visible Sensor</b> <a href="http://www.mdpi.com/1424-8220/17/7/1689" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Haopeng Zhang, <a href="https://weiquanmao.github.io" target="_blank">Quanmao Wei</a> and Zhiguo Jiang
                </i></font>
                <br>
                Sensors, 2017
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="http://www.mdpi.com/1424-8220/17/7/1689/pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangSensors17Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangSensors17Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhangSensors17Abs" class="abstract" style="display: none;">
                In this paper, a novel 3D reconstruction framework is proposed to recover the 3D structural model of a space object from its multi-view images captured by a visible sensor. Given an image sequence, this framework first estimates the relative camera poses and recovers the depths of the surface points by the structure from motion (SFM) method, then the patch-based multi-view stereo (PMVS) algorithm is utilized to generate a dense 3D point cloud. To resolve the wrong matches arising from the symmetric structure and repeated textures of space objects, a new strategy is introduced, in which images are added to SFM in imaging order. Meanwhile, a refining process exploiting the structural prior knowledge that most sub-components of artificial space objects are composed of basic geometric shapes is proposed and applied to the recovered point cloud. The proposed reconstruction framework is tested on both simulated image datasets and real image datasets. Experimental results illustrate that the recovered point cloud models of space objects are accurate and have a complete coverage of the surface. Moreover, outliers and points with severe noise are effectively filtered out by the refinement, resulting in an distinct improvement of the structure and visualization of the recovered points.
              </p>
              <pre xml:space="preserve" id="zhangSensors17Bib" class="bibtex" style="display: none;">
@article{zhangSensors17,
  author  = {Haopeng Zhang and Quanmao Wei and Zhiguo Jiang},
  title   = {3D Reconstruction of Space Objects from Multi-Views by A Visible Sensor},
  journal = {Sensors},
  volume  = {17},
  year    = {2017},
  number  = {7},
  article number = {1689},
  url     = {http://www.mdpi.com/1424-8220/17/7/1689},
  issn    = {1424-8220},
  doi     = {10.3390/s17071689}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhangSensors17Abs');
                hideblock('zhangSensors17Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_wei_igta_2017.jpg" alt="Results of article_wei_igta_17" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Spacecraft Component Detection in Point Clouds</b> <a href="https://link.springer.com/chapter/10.1007/978-981-10-7389-2_21" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  <a href="https://weiquanmao.github.io" target="_blank">Quanmao Wei</a>, Zhiguo Jiang, Haopeng Zhang and Nie Shanlan
                </i></font>
                <br>
                Image and Graphics Technologies and Applications (IGTA), 2017
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="https://link.springer.com/content/pdf/10.1007%2F978-981-10-7389-2_21.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('weiIGTA17Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('weiIGTA17Bib')">BibTeX</a> &nbsp;
                <i class="fa fa-code"></i> <a href="https://github.com/weiquanmao/PCF" target="_blank">Code</a> &nbsp
              </p>
              <p id="weiIGTA17Abs" class="abstract" style="display: none;">
                Component detection of spacecraft is significant for on-orbit operation and space situational awareness. Solar wings and main body are the major components of most spacecrafts, and can be described by geometric primitives like planes, cuboid or cylinder. Based on this prior, pipeline to automatically detect the basic components of spacecraft in 3D point clouds is presented, in which planes, cuboid and cylinder are successively detected. The planar patches are first detected as possible solar wings in point clouds of the recorded object. As for detection of the main body, inferring a cuboid main body from the detected patches is first attempted, and a further attempt to extract a cylinder main body is made if no cuboid exists. Dimensions are estimated for each component. Experiments on satellite point cloud data that are recovered by image-based reconstruction demonstrated effectiveness and accuracy of this pipeline.
              </p>
              <pre xml:space="preserve" id="weiIGTA17Bib" class="bibtex" style="display: none;">
@inproceedings{weiIGTA17,
  author    = {Quanmao Wei and Zhiguo Jiang and Haopeng Zhang and Shanlan Nie},
  editor    = {Yongtian Wang and Shengjin Wang and Yue Liu and Jian Yang
               and Xiaoru Yuan and Ran He and Henry Been-Lirn Duh},
  title     = {Spacecraft Component Detection in Point Clouds},
  booktitle = {Advances in Image and Graphics Technologies},
  year      = {2017},
  publisher = {Springer Singapore},
  address   = {Singapore},
  pages     = {210—-218},
  isbn      = {978-981-10-7389-2}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('weiIGTA17Abs');
                hideblock('weiIGTA17Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_cai_rs_2017.jpg" alt="flowchart_of_cai_rs_17" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Airport Detection Using End-to-End Convolutional Neural Network with Hard Example Mining</b> <a href="http://www.mdpi.com/2072-4292/9/11/1198" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Bowen Cai, Zhiguo Jiang, Haopeng Zhang, Danpei Zhao and Yuan Yao
                </i></font>
                <br>
                Remote Sensing, 2017
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="http://www.mdpi.com/2072-4292/9/11/1198/pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('caiRS2017Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('caiRS2017Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="caiRS2017Abs" class="abstract" style="display: none;">
                  Deep convolutional neural network (CNN) achieves outstanding performance in the field of target detection. As one of the most typical targets in remote sensing images (RSIs), airport has attracted increasing attention in recent years. However, the essential challenge for using deep CNN to detect airport is the great imbalance between the number of airports and background examples in large-scale RSIs, which may lead to over-fitting. In this paper, we develop a hard example mining and weight-balanced strategy to construct a novel end-to-end convolutional neural network for airport detection. The initial motivation of the proposed method is that backgrounds contain an overwhelming number of easy examples and a few hard examples. Therefore, we design a hard example mining layer to automatically select hard examples by their losses, and implement a new weight-balanced loss function to optimize CNN. Meanwhile, the cascade design of proposal extraction and object detection in our network releases the constraint on input image size and reduces spurious false positives. Compared with geometric characteristics and low-level manually designed features, the hard example mining based network could extract high-level features, which is more robust for airport detection in complex environment. The proposed method is validated on a multi-scale dataset with complex background collected from Google Earth. The experimental results demonstrate that our proposed method is robust, and superior to the state-of-the-art airport detection models.
              </p>
              <pre xml:space="preserve" id="caiRS2017Bib" class="bibtex" style="display: none;">
@article{caiRS17,
  author  = {Bowen Cai and Zhiguo Jiang and Haopeng Zhang and Danpei Zhao and Yuan Yao},
  title   = {Airport Detection Using End-to-End Convolutional Neural Network
             with Hard Example Mining},
  journal = {Remote Sensing},
  volume  = {9},
  year    = {2017},
  number  = {11},
  article number = {1198},
  url     = {http://www.mdpi.com/2072-4292/9/11/1198},
  issn    = {2072-4292},
  doi     = {10.3390/rs9111198}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('caiRS2017Abs');
                hideblock('caiRS2017Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_cai_igarss_2017.jpg" alt="flowchart_of_cai_igarss_17" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Training Deep Convolution Neural Network with Hard Example Mining for Airport Detection</b> <a href="http://ieeexplore.ieee.org/document/8127089/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Bowen Cai, Zhiguo Jiang, Haopeng Zhang, Yuan Yao and Jie Huang
                </i></font>
                <br>
                IEEE International Conference on Geoscience and Remote Sensing Symposium (IGARSS), 2017
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8127089" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('caiIGARSS2017Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('caiIGARSS2017Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="caiIGARSS2017Abs" class="abstract" style="display: none;">
                The geometrical characteristic and low-level manually designed features are usually used to detect airports in optical remote sensing images. But it is insufficient to describe airport in low resolution and illumination environment. This paper presents a hard example mining algorithm to train the end-to-end deep convolutional neural network for airport detection in complex situation. Compared with conventional airport detection methods which design specific low-level manually designed features for high-resolution remote sensing images, an end-to-end network can mine the general characteristic among the training samples and learn high-level features in multi-scale and multi-view remote sensing images. Meanwhile, an automatic hard example mining principle is introduced to make training more efficiently and accurately. The proposed method is validated on a multi-scale and multi-view dataset collected from Google Earth. The experimental results demonstrate that the proposed method is robust and efficient, and superior to the state-of-the-art airport detection models.
              </p>
              <pre xml:space="preserve" id="caiIGARSS2017Bib" class="bibtex" style="display: none;">
@inproceedings{caiIGARSS17,
  author    = {Bowen Cai and Zhiguo Jiang and Haopeng Zhang and Yuan Yao and Jie Huang},
  booktitle = {2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
  title     = {Training Deep Donvolution Neural Network
               with Hard Example Mining for Airport Detection},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {862—-865},
  doi       = {10.1109/IGARSS.2017.8127089},
  issn      = {},
  month     = {July}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('caiIGARSS2017Abs');
                hideblock('caiIGARSS2017Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zheng_spiemi_2017.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Content-based Histopathological Image Retrieval for Whole Slide Image Database Using Binary Codes</b> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10140/1/Content-based-histopathological-image-retrieval-for-whole-slide-image-database/10.1117/12.2253988.full?SSO=1" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yushan Zheng, Zhiguo Jiang, Yibing Ma, Haopeng Zhang, Fengying Xie, Huaqiang Shi and Yu Zhao
                </i></font>
                <br>
                SPIE Medical Imaging, 2017
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_spiemi_2017.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengSPIEMI2017Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengSPIEMI2017Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhengSPIEMI2017Abs" class="abstract" style="display: none;">
                Content-based image retrieval (CBIR) has been widely researched for medical images. In application of histo- pathological images, there are two issues that need to be carefully considered. The one is that the digital slide is stored in a spatially continuous image with a size of more than 10K x 10K pixels. The other is that the size of query image varies in a large range according to different diagnostic conditions. It is a challenging work to retrieve the eligible regions for the query image from the database that consists of whole slide images (WSIs). In this paper, we proposed a CBIR framework for the WSI database and size-scalable query images. Each WSI in the database is encoded and stored in a matrix of binary codes. When retrieving, the query image is first encoded into a set of binary codes and analyzed to pre-choose a set of regions from database using hashing method. Then a multi-binary-code-based similarity measurement based on hamming distance is designed to rank proposal regions. Finally, the top relevant regions and their locations in the WSIs along with the diagnostic information are returned to assist pathologists in diagnoses. The effectiveness of the proposed framework is evaluated in a fine-annotated WSIs database of epithelial breast tumors. The experimental results show that proposed framework is both effective and efficiency for content-based whole slide image retrieval.
              </p>
              <pre xml:space="preserve" id="zhengSPIEMI2017Bib" class="bibtex" style="display: none;">
@inproceedings{zhengSPIEME17,
  title     = {Content-based Histopathological Image Retrieval
               for Whole Slide Image Database Using Binary Codes},
  author    = {Yushan Zheng and Zhiguo Jiang and Yibing Ma and Haopeng Zhang
               and Fengying Xie and Huaqiang Shi and Yu Zhao},
  booktitle = {SPIE Medical Imaging},
  doi       = {doi.org/10.1117/12.2253988},
  pages     = {1014013},
  year      = {2017}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhengSPIEMI2017Abs');
                hideblock('zhengSPIEMI2017Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
        </tbody></table>

        <hr />
        <!-- -------------------------------------------- -->
        <!-- -------------------------------------------- -->
        <h3>2016</h3>
        <table><tbody>
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/airticle_wu_igarss_2016.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Semi-supervised Conditional Random Field for hyperspectral remote sensing image classification</b> <a href="https://ieeexplore.ieee.org/document/7729675/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Junfeng Wu, Zhiguo Jiang, Haopeng Zhang, Bowen Cai and Quanmao Wei
                </i></font>
                <br>
                IEEE International Conference on Geoscience and Remote Sensing Symposium (IGARSS),2016
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('wuIGARSS2016Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('wuIGARSS2016Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="wuIGARSS2016Abs" class="abstract" style="display: none;">
                  <b>Conditional Random Field</b>(CRF) has been successfully applied to the <b>hyperspectral image classification</b>. However, it suffers from the availability of large amount of labeled pixels, which is labor- and time-consuming to obtain in practice. In this paper, a semi-supervised CRF(ssCRF) is proposed <b>for hyperspectral image classification</b> with limited labeled pixels. Laplacian Support Vector Machine(LapSVM), after extended into the composite kernel type, is defined as the association potential. And the Potts model is utilized as the interaction potential. The ssCRF is evaluated on the two benchmarks and the results show the effectiveness of ssCRF.
              </p>
              <pre xml:space="preserve" id="wuIGARSS2016Bib" class="bibtex" style="display: none;">
@inproceedings{wuIGARSS2016,
  title     = {Semi-supervised Conditional Random Field for 
               hyperspectral remote sensing image classification},
  author    = {Junfeng Wu and Zhiguo Jiang and Haopeng Zhang 
               and Bowen Cai and Quanmao Wei},
  booktitle = {2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
  doi       = {10.1109/IGARSS.2016.7729675},
  pages     = {2614-2617},
  year      = {2016}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('wuIGARSS2016Abs');
                hideblock('wuIGARSS2016Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/airticle_luo_el_2016.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Multi-scale orderless cross-regions-pooling of deep attributes for image retrieval</b> <a href="https://ieeexplore.ieee.org/document/7405383/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Jianwei Luo, Zhiguo Jiang and Jianguo Li 
                </i></font>
                <br>
                Electronics Letters,2016
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('luoel2016Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('luoel2016Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="luoel2016Abs" class="abstract" style="display: none;">
                How to represent an image is an essential problem of the image retrieval task. To build a powerful image representation, a novel method named cross-regions-pooling (CRP) combining two key ingredients is proposed: (i) region proposals detected by objectness detection technique; (ii) deep attributes (DA), i.e. the outputs of the softmax layer of off-the-shelf convolutional neural network pre-trained on a large-scale dataset. The ultimate representation of an image is the aggregation (e.g. max-pooling) of DA extracted from all the regions. In addition, a multi-scale orderless pooling strategy considering layout of contexts of an image is proposed to integrate with CRP to improve the image representation. Experimental results on standard benchmarks demonstrate superiority of the proposed method over state-of-the-arts.
              </p>
              <pre xml:space="preserve" id="luoel2016Bib" class="bibtex" style="display: none;">
@article{Luo2016Multi,
  title={Multi-scale orderless cross-regions-pooling of deep attributes for image retrieval},
  author={Luo, Jianwei and Jiang, Zhiguo and Li, Jianguo},
  journal={Electronics Letters},
  volume={52},
  number={4},
  pages={276-277},
  year={2016},
}

              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('luoel2016Abs');
                hideblock('luoel2016Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_ma_cmpb_2018.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Breast Histopathological Image Retrieval Based on Latent Dirichlet Allocation</b> <a href="http://ieeexplore.ieee.org/document/7572100/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Yibing Ma, Zhiguo Jiang, Haopeng Zhang, Fengying Xie, Yushan Zheng, Huaqiang Shi and Yu Zhao
                </i></font>
                <br>
                  IEEE Journal of Biomedical and Health Informatics, 2016
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('maJBHI2016Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('maJBHI2016Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="maJBHI2016Abs" class="abstract" style="display: none;">
                  In the field of pathology, whole slide image (WSI) has become the major carrier of visual and diagnostic information. Content-based image retrieval among WSIs can aid the diagnosis of an unknown pathological image by finding its similar regions in WSIs with diagnostic information. However, the huge size and complex content of WSI pose several challenges for retrieval. In this paper, we propose an unsupervised, accurate, and fast retrieval method for a breast histopathological image. Specifically, the method presents a local statistical feature of nuclei for morphology and distribution of nuclei, and employs the Gabor feature to describe the texture information. The latent Dirichlet allocation model is utilized for high-level semantic mining. Locality-sensitive hashing is used to speed up the search. Experiments on a WSI database with more than 8000 images from 15 types of breast histopathology demonstrate that our method achieves about 0.9 retrieval precision as well as promising efficiency. Based on the proposed framework, we are developing a search engine for an online digital slide browsing and retrieval platform, which can be applied in computer-aided diagnosis, pathology education, and WSI archiving and management.
                </p>
                <pre xml:space="preserve" id="maJBHI2016Bib" class="bibtex" style="display: none;">
@article{maJBHI2016, 
  author  = {Yibing Ma and Zhiguo Jiang and Haopeng Zhang and Fengying Xie 
             and Yushan Zheng and Huaqiang Shi and Yu Zhao and Jun Shi}, 
  journal = {IEEE Journal of Biomedical and Health Informatics}, 
  title   = {Breast Histopathological Image Retrieval Based on Latent Dirichlet Allocation}, 
  year    = {2017}, 
  volume  = {21}, 
  number  = {4}, 
  pages   = {1114-1123}, 
  doi     = {10.1109/JBHI.2016.2611615}, 
  ISSN    = {2168-2194}, 
  month   = {July}
}    
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('maJBHI2016Abs');
                hideblock('maJBHI2016Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zhang_igta_2016.jpg" alt="Results of article_zhang_igta_16" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Pose Estimation of Space Objects Based on Hybrid Feature Matching of Contour Points</b> <a href="https://link.springer.com/chapter/10.1007/978-981-10-2260-9_21" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Xin Zhang, Haopeng Zhang, <a href="https://weiquanmao.github.io" target="_blank">Quanmao Wei</a> and Zhiguo Jiang
                </i></font>
                <br>
                Image and Graphics Technologies and Applications (IGTA), 2016
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="https://link.springer.com/content/pdf/10.1007%2F978-981-10-2260-9_21.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangIGTA16Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangIGTA16Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhangIGTA16Abs" class="abstract" style="display: none;">
                This paper presents an improved pose estimation algorithm for vision-based space objects. The major weakness of most existing methods is limited convergence radius. In most cases they ignore the influence of translation, only focusing on rotation parameters. To breakthrough these limits, we utilizes hybrid local image features to explicitly establish 2D-3D correspondences between the input image and 3D model of space objects, and then estimate rotation and translation parameters based on the correspondences. Experiments with simulated models are carried out, and the results show that our algorithm can successfully estimate the pose of space objects with large convergence radius and high accuracy.
              </p>
              <pre xml:space="preserve" id="zhangIGTA16Bib" class="bibtex" style="display: none;">
@inproceedings{zhangIGTA16,
  author    = {Xin Zhang and Haopeng Zhang and Quanmao Wei and Zhiguo Jiang},
  editor    = {Tieniu Tan and Guoping Wang and Shengjin Wang and Yue Liu
               and Xiaoru Yuan and Ran He and Sheng Li},
  title     = {Pose Estimation of Space Objects Based on Hybrid Feature Matching
               of Contour Points},
  booktitle = {Advances in Image and Graphics Technologies},
  year      = {2016},
  publisher = {Springer Singapore},
  address   = {Singapore},
  pages     = {184—-191},
  isbn      = {978-981-10-2260-9}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhangIGTA16Abs');
                hideblock('zhangIGTA16Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zhang_jbuaa_2016.jpg" alt="Results of article_zhang_jbuaa_16" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Sequential-image-based Space Object 3D Reconstruction</b> <a href="http://doi.cnki.net/Resolution/Handler?doi=10.13700/j.bh.1001-5965.2015.0117" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Haopeng Zhang, <a href="https://weiquanmao.github.io" target="_blank">Quanmao Wei</a>, Wei Zhang, Junfeng Wu and Zhiguo Jiang
                </i></font>
                <br>
                Journal of BUAA, 2016 <i>[In Chinese]</i>
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="http://bhxb.buaa.edu.cn/CN/article/downloadArticleFile.do?attachType=PDF&id=13381" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhangJBUAA16Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhangJBUAA16Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhangJBUAA16Abs" class="abstract" style="display: none;">
                Space object 3D reconstruction is of important significance for both space situational awareness and theoretical study. A new structure from motion method was proposed to avoid the reconstruction error caused by the symmetrical structure and similar texture of space targets. In this method, new images were added sequentially for reconstruction using the imaging time as a priori knowledge. In addition, image simulation of space target and ground imaging simulation experiment were carried out for the lack of space target image data. And experiments on the simulated space target images, in which the motion analysis results are accurate and robust to noise, and the recovery 3D point cloud can express the structural information of the target to a certain extent, have demonstrated the effectiveness of the approach proposed, and the boundary conditions of multi-frame-image for 3D reconstruction are acquired as well.
              </p>
              <pre xml:space="preserve" id="zhangJBUAA16Bib" class="bibtex" style="display: none;">
@article{zhangJBUAA16,
  language = {Chinse},
  author   = {Haopeng Zhang and Quanmao Wei and Wei Zhang and Junfeng Wu and Zhiguo Jiang},
  title    = {Sequential-image-based Space Object 3D Reconstruction},
  journal  = {Journal of Beijing University of Aeronautics and Astronautics},
  year     = {2016},
  volume   = {42},
  number   = {2},
  pages    = {273--279},
  issn     = {10015965},
  URL      = {http://dx.doi.org/10.13700/j.bh.1001-5965.2015.0117}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhangJBUAA16Abs');
                hideblock('zhangJBUAA16Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_yao_igarss_2016.jpg" alt="Results of article_yao_igarss_2016" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>High-resolution Optical Satellite Image Simulation of Ship Target in Large Sea Scenes</b> <a href="http://ieeexplore.ieee.org/document/7729314/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yuan Yao, Zhiguo Jiang and Haopeng Zhang
                </i></font>
                <br>
                IEEE International Conference on Geoscience and Remote Sensing Symposium (IGARSS), 2016
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_yao_igarss_2016.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('yaoIGARSS16Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('yaoIGARSS16Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="yaoIGARSS16Abs" class="abstract" style="display: none;">
                Ship target detection in optical remote sensing images has attracted more and more attention in the field of remote sensing. The ship target detection technology of optical remote sensing images is vulnerable to many factors, while the real data are difficult to contain various elements. In order to obtain the various situations in the large sea scenes, we develop a simulation system for high-resolution optical remote sensing image of ship targets. The simulated images with different sea states, cloud conditions, target types and imaging conditions can support the evaluation and comparison of ship detection algorithms as well as other tasks in remote sensing image analysis.
              </p>
              <pre xml:space="preserve" id="yaoIGARSS16Bib" class="bibtex" style="display: none;">
@inproceedings{yaoIGARSS16,
  author    = {Yuan Yao and Zhiguo Jiang and Haopeng Zhang},
  booktitle = {2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
  title     = {High-resolution Optical Satellite Image Simulation of Ship Target
               in Large Sea Scenes},
  year      = {2016},
  month     = {July},
  volume    = {},
  number    = {},
  pages     = {1241—-1244},
  doi       = {10.1109/IGARSS.2016.7729314}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('yaoIGARSS16Abs');
                hideblock('yaoIGARSS16Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
        </tbody></table>

        <hr />
        <!-- -------------------------------------------- -->
        <!-- -------------------------------------------- -->
        <h3>2015</h3>
        <table><tbody>
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Haopeng_sci_2015.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Satellite recognition and pose estimation using homeomorphic manifold analysis</b> <a href="https://ieeexplore.ieee.org/document/7073533/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Haopeng Zhang, Zhiguo Jiang, Elgammal A
                </i></font>
                <br>
                  IEEE Transactions on Aerospace and Electronic Systems, 2015
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2015Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2015Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="HaopengSCI2015Abs" class="abstract" style="display: none;">
                We propose a novel monocular vision-based framework for both satellite recognition and pose estimation, using homeomorphic manifold analysis. We use a unified conceptual manifold to represent continuous pose variation of all satellites in the visual input space, learn nonlinear function mapping from conceptual manifold representation to visual inputs, and decompose discrete category variation in the mapping coefficient space. Experimental results on a simulated image data set show the effectiveness and robustness of our approach.
                </p>
              <pre xml:space="preserve" id="HaopengSCI2015Bib" class="bibtex" style="display: none;">
@article{HaopengTAES2015,
  title   = {Satellite recognition and pose estimation using homeomorphic manifold analysis},
  author  = {Haopeng Zhang, Zhiguo Jiang, Elgammal A},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  pages   = {785 - 792},
  year    = {2015},
  issn    = {0018-9251},
  url     = {https://ieeexplore.ieee.org/document/7073533/},
}      
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('HaopengSCI2015Abs');
                hideblock('HaopengSCI2015Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Haopeng_sci_2015_1.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b> Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation</b> <a href="https://www.sciencedirect.com/science/article/pii/S1077314215000715?via%3Dihub" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Haopeng Zhang, El-Gaaly T, Elgammal A, Zhiguo Jiang
                </i></font>
                <br>
                  Computer Vision and Image Understanding, 2015
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2015_1Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2015_1Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="HaopengSCI2015_1Abs" class="abstract" style="display: none;">
                Due to large variations in shape, appearance, and viewing conditions, object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parameterized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we develop a novel framework to jointly solve the three challenging recognition sub-problems, by explicitly modeling the deformations of object manifolds and factorizing it in a view-invariant space for recognition. We perform extensive experiments on several challenging datasets and achieve state-of-the-art results.
                </p>
              <pre xml:space="preserve" id="HaopengSCI2015_1Bib" class="bibtex" style="display: none;">
@article{HaopengCVIU2015,
  title   = {Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation},
  author  = {Haopeng Zhang, El-Gaaly T, Elgammal A, Zhiguo Jiang},
  journal = {Computer Vision and Image Understanding},
  volume  = {139},
  pages   = {89 - 103},
  year    = {2015}
  url     = {https://www.sciencedirect.com/science/article/pii/S1077314215000715?via%3Dihub},
}      
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('HaopengSCI2015_1Abs');
                hideblock('HaopengSCI2015_1Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
        
        <tr> <!-- An Paper -->
          <td width="22%" valign="top"><p>
            <img src="images/src/article_Haopeng_sci_2015_2.jpg" alt="Result" width="200">
          </p></td>
          <td width="78%" valign="top">
            <p>
              <b> Vision-based pose estimation for space objects by Gaussian process regression</b> <a href="https://ieeexplore.ieee.org/document/7118908/" target="_blank"><i class="fa fa-external-link"></i></a>
              <br>
              <font size="3pt" face="Georgia"><i>
                  Haopeng Zhang, Zhiguo Jiang, Yuan Yao, et al
              </i></font>
              <br>
                IEEE Aerospace Conference, 2015
              <br>
              <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2015_2Abs')">Abstract</a> &nbsp;
              <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2015_2Bib')">BibTeX</a> &nbsp;
            </p>
            <p id="HaopengSCI2015_2Abs" class="abstract" style="display: none;">
                We address the problem of vision-based pose estimation for space objects, which is to estimate the relative pose of a target spacecraft using imaging sensors. We develop a novel monocular vision-based method by employing Gaussian process regression (GPR) to solve pose estimation for space objects. GPR is a powerful regression model for predicting continuous quantities, and can easily obtain and express uncertainty. Assuming that the regression function mapping from the image (or feature) of the target spacecraft to its pose follows a Gaussian process (GP) properly parameterized by a mean function and a covariance function, the predictive equations can be easily obtained by a maximum-likelihood approach when training data are given. The mean value of the predicted output (i.e. the estimated pose) and its variance (which indicates the uncertainty) can be computed via these explicit formulations. Besides, we also introduce a manifold constraint to the output of GPR model to improve its performance for spacecraft pose estimation. We performed extensive experiments on a simulated image dataset that contains satellite images of 1D and 2D pose variation, as well as images with noises and different lighting conditions. Experimental results validate the effectiveness and robustness of our approach. Our model can not only estimate the pose angles of space objects but also provide the uncertainty of the estimated values which may be used to choose convincing results in applications.
              </p>
            <pre xml:space="preserve" id="HaopengSCI2015_2Bib" class="bibtex" style="display: none;">
@article{HaopengAC2015,
title   = {Vision-based pose estimation for space objects by Gaussian process regression},
author  = {Haopeng Zhang, Zhiguo Jiang, Yuan Yao, et al},
journal = {Aerospace Conference, 2015 IEEE},
pages   = {1 - 9},
year    = {2015},
doi     = {10.1109/AERO.2015.7118908},
issn    = {1095-323X}
url     = {https://ieeexplore.ieee.org/document/7118908/},
}      
            </pre>
            <script language="javascript" type="text/javascript" xml:space="preserve">
              hideblock('HaopengSCI2015_2Abs');
              hideblock('HaopengSCI2015_2Bib');
            </script>
          </td>
        </tr> <!-- Paper End Here -->
          
        </tbody></table>

        <hr />
        <!-- -------------------------------------------- -->
        <!-- -------------------------------------------- -->
        <h3>2014</h3>
        <table><tbody>
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                  <img src="images/src/article_wu_EL_2014.jpg" alt="Results of article_yao_igarss_2016" width="200">
              </p></td>
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>Composite kernels conditional random fields for remote-sensing image classification</b> <a href="https://www.crossref.org/iPage?doi=10.1049%2Fel.2014.1964" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Junfeng Wu，Zhiguo Jiang, Jianwei Luo and Haopeng Zhang
                  </i></font>
                  <br>
                  Electronic Letters, 2014
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('wuElectron Lett2014Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('wuElectron Lett2014Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="wuElectron Lett2014Abs" class="abstract" style="display: none;">
                    The problem of classifying a remote-sensing image by specifically labelling each pixel in the image is addressed. A novel method, named composite kernels conditional random field (CKCRF), which embeds multiple kernels into a classical CRFs model is proposed. Rather than manually selecting kernel-like KCRF, CKCRFs chooses the appropriate kernel by training. Moreover, a genetic programming-based decision-level fusion framework is proposed to tackle the problem of feature selection. It can select the appropriate features suitable to each category. Evaluations show that CKCRFs outperform CRFs and KCRFs, and CKCRFs with the fusion scheme is better than that without the fusion step.
                </p>
                <pre xml:space="preserve" id="wuElectron Lett2014Bib" class="bibtex" style="display: none;">
  @inproceedings{wuElectron Lett2014,
    title     = {Composite kernels conditional random fields for remote-sensing image classification},
    author    = {Junfeng Wu and Zhiguo Jiang and Jianwei Luo and Haopeng Zhang},
    booktitle = {2014 ELECTRONICS LETTERS(Electron Lett)},
    doi       = {10.1049/el.2014.1964},
    pages     = {1589-1590},
    year      = {2014}
  }
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('wuElectron Lett2014Abs');
                  hideblock('wuElectron Lett2014Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->

            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/airticle_luo_icpr_2014.jpg" alt="Result" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>Learning Semantic Binary Codes by Encoding Attributes for Image Retrieval </b> <a href="http://xueshu.baidu.com/s?wd=paperuri%3A%287ca2be495d9efab025919b428430ae7c%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F6976768%2F&ie=utf-8&sc_us=5815303703915717955" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                    Jianwei Luo and Zhiguo Jiang
                  </i></font>
                  <br>
                  International Conference on Pattern Recognition ,2014
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('luoicpr2014Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('luoicpr2014Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="luoicpr2014Abs" class="abstract" style="display: none;">
                  This paper addresses the problem of learning semantic compact binary codes for efficient retrieval in large-scale image collections. Our contributions are three-fold. Firstly, we introduce semantic codes, of which each bit corresponds to an attribute that describes a property of an object (e.g. dogs have furry). Secondly, we propose to use matrix factorization (MF) to learn the semantic codes by encoding attributes. Unlike traditional PCA-based encoding methods which quantize data into orthogonal bases, MF assumes no constraints on bases, and this scheme is coincided with that attributes are correlated. Finally, to augment semantic codes, MF is extended to encode extra non-semantic codes to preserve similarity in origin data space. Evaluations on a-Pascal dataset show that our method is comparable to the state-of-the-art when using Euclidean distance as ground truth, and even outperforms state-of-the-art when using class label as ground truth. Furthermore, in experiments, our method can retrieve images that share the same semantic properties with the query image, which can be used to other vision tasks, e.g. re-training classifiers.
                </p>
                <pre xml:space="preserve" id="luoicpr2014Bib" class="bibtex" style="display: none;">
@inproceedings{Luo2014Learning,
  title={Learning Semantic Binary Codes by Encoding Attributes for Image Retrieval},
  author={Luo, Jianwei and Jiang, Zhiguo},
  booktitle={International Conference on Pattern Recognition},
  pages={279-284},
  year={2014},
}
            </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('luoicpr2014Abs');
                  hideblock('luoicpr20142014Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
            
            
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/article_Haopeng_sci_2014.jpg" alt="Result" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b> Multi-view space object recognition and pose estimation based on kernel regression</b> <a href="https://www.sciencedirect.com/science/article/pii/S1000936114000533" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang, Zhiguo Jiang
                  </i></font>
                  <br>
                   Chinese Journal of Aeronautics, 2014
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2014Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2014Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="HaopengSCI2014Abs" class="abstract" style="display: none;">
                    We address the problem of vision-based pose estimation for space objects, which is to estimate the relative pose of a target spacecraft using imaging sensors. We develop a novel monocular vision-based method by employing Gaussian process regression (GPR) to solve pose estimation for space objects. GPR is a powerful regression model for predicting continuous quantities, and can easily obtain and express uncertainty. Assuming that the regression function mapping from the image (or feature) of the target spacecraft to its pose follows a Gaussian process (GP) properly parameterized by a mean function and a covariance function, the predictive equations can be easily obtained by a maximum-likelihood approach when training data are given. The mean value of the predicted output (i.e. the estimated pose) and its variance (which indicates the uncertainty) can be computed via these explicit formulations. Besides, we also introduce a manifold constraint to the output of GPR model to improve its performance for spacecraft pose estimation. We performed extensive experiments on a simulated image dataset that contains satellite images of 1D and 2D pose variation, as well as images with noises and different lighting conditions. Experimental results validate the effectiveness and robustness of our approach. Our model can not only estimate the pose angles of space objects but also provide the uncertainty of the estimated values which may be used to choose convincing results in applications.
                  </p>
                <pre xml:space="preserve" id="HaopengSCI2014Bib" class="bibtex" style="display: none;">
    @article{HaopengCJA2014,
    title   = {Multi-view space object recognition and pose estimation based on kernel regression},
    author  = {Haopeng Zhang, Zhiguo Jiang},
    journal = {Chinese Journal of Aeronautics},
    pages   = {1233 - 1241},
    year    = {2014},
    volume  = {27},
    url     = {https://www.sciencedirect.com/science/article/pii/S1000936114000533},
    }      
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('HaopengSCI2014Abs');
                  hideblock('HaopengSCI2014Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/article_Haopeng_sci_2014_1.jpg" alt="Result" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b> Manifold representation of multi-view images</b> <a href="https://www.researchgate.net/publication/288287272_Manifold_representation_of_multi-view_images" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang, Zhiguo Jiang
                  </i></font>
                  <br>
                    Journal of Computational Information Systems,2014
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2014_1Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2014_1Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="HaopengSCI2014_1Abs" class="abstract" style="display: none;">
                    Abstract Images of the same object lie on a low-dimensional manifold (view manifold) in the visual space. View manifolds can be used to represent viewpoint variation of multi-view images in the embedding space, and can be very helpful to multi-view object detection, classification, and viewpoint estimation. In this paper, we introduce a conceptual manifold as a common representation of all view manifolds. In order to evaluate the performance of the conceptual manifold representation, we learn a generative model that can map from the manifold representation to visual inputs for the tasks of arbitrary view image synthesis and viewpoint estimation. We did experiments on COIL-20 dataset, and compared with popular manifold learning methods. Experimental results show that our conceptual manifold representation can effectively describe the viewpoint variation of multi-view images with strong robustness, and outperform the view manifolds learned by popular manifold learning methods.
                  </p>
                <pre xml:space="preserve" id="HaopengSCI2014_1Bib" class="bibtex" style="display: none;">
    @article{HaopengJCIS2014,
    title   = {Manifold representation of multi-view images},
    author  = {Haopeng Zhang, Zhiguo Jiang},
    journal = {Journal of Computational Information Systems},
    pages   = {4867 - 4876},
    year    = {2014},
    doi     = {10.12733/jcis10624},
    url     = {https://www.researchgate.net/publication/288287272_Manifold_representation_of_multi-view_images},
    }      
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('HaopengSCI2014_1Abs');
                  hideblock('HaopengSCI2014_1Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/article_Haopeng_sci_2014_2.jpg" alt="Result" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b> Constrained kernel regression for pose estimation</b> <a href="https://ieeexplore.ieee.org/document/6729319/?arnumber=6729319&tag=1" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang, Zhiguo Jiang
                  </i></font>
                  <br>
                    Electronic letters, 2014
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2014_2Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2014_2Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="HaopengSCI2014_2Abs" class="abstract" style="display: none;">
                    A constrained kernel regression model is proposed to solve the problem of one-dimensional (1D) pose estimation. Unlike the traditional kernel regression model, a circular constraint is applied to the output of the regression function, i.e. using 2D coordinates on a unit circle as output instead of 1D pose angles from 0 to 360°. The experimental results show that with this constraint, the performance of kernel regression on the 1D pose estimation can be improved significantly, and the constrained kernel regression model can run in real-time.
                  </p>
                <pre xml:space="preserve" id="HaopengSCI2014_2Bib" class="bibtex" style="display: none;">
    @article{HaopengEL2014,
    title   = {Constrained kernel regression for pose estimation},
    author  = {Haopeng Zhang, Zhiguo Jiang},
    journal = {Electronics Letters},
    pages   = {77 - 79},
    year    = {2014},
    issn    = {0013-5194},
    doi     = { 10.1049/el.2013.2071},
    url     = {https://ieeexplore.ieee.org/document/6729319/?arnumber=6729319&tag=1},
    }      
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('HaopengSCI2014_2Abs');
                  hideblock('HaopengSCI2014_2Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zheng_icip_2014.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Retrieval of Pathology Image for Breast Cancer Using PLSA Model Based on Texture and Pahological Features</b> <a href="https://ieeexplore.ieee.org/document/7025467/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yushan Zheng, Zhiguo Jiang, Jun Shi and Yibing Ma
                </i></font>
                <br>
                IEEE International Conference on Image Processing (ICIP), 2014
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_icip_2014.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengICIP2014Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengICIP2014Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhengICIP2014Abs" class="abstract" style="display: none;">
                Content-based image retrieval (CBIR) for digital pathology slides is of clinical use for breast cancer aided diagnosis. One of the largest challenges in CBIR is feature extraction. In this paper, we propose a novel pathology image retrieval method for breast cancer, which aims to characterize the pathology image content through texture and pathological features and further discover the latent high-level semantics. Specifically, the proposed method utilizes block Gabor features to describe the texture structure, and simultaneously designs nucleus-based pathological features to describe morphological characteristics of nuclei. Based on these two kinds of local feature descriptors, two codebooks are built to learn the probabilistic latent semantic analysis (pLSA) models. Consequently, each image is represented by the topics of pLSA models which can reveal the semantic concepts. Experimental results on the digital pathology image database for breast cancer demonstrate the feasibility and effectiveness of our method.
              </p>
              <pre xml:space="preserve" id="zhengICIP2014Bib" class="bibtex" style="display: none;">
@article{zhengICIP14,
  title     = {Retrieval of Pathology Image for Breast Cancer Using PLSA Model
               Based on Texture and Pathological Features},
  author    = {Yushan Zheng and Zhiguo Jiang and Jun Shi and Yibing Ma},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  pages     = {2304--2308},
  year      = {2014}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhengICIP2014Abs');
                hideblock('zhengICIP2014Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_zheng_igta_2014.jpg" alt="Flowchart" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Pathology Image Retrieval by Block LBP Based PLSA Model with Low-Rank and Sparse Matrix Decomposition</b> <a href="https://link.springer.com/chapter/10.1007/978-3-662-45498-5_36" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yushan Zheng, Zhiguo Jiang, Jun Shi and Yibing Ma
                </i></font>
                <br>
                Image and Graphics Technologies and Applications (IGTA), 2014
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_igta_2014.pdf" target="_blank">PDF</a> &nbsp;
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('zhengIGTA2014Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('zhengIGTA2014Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="zhengIGTA2014Abs" class="abstract" style="display: none;">
                Content-based image retrieval (CBIR) is widely used in Computer Aided Diagnosis (CAD) systems which can aid pathologist to make reasonable decision by querying the slides with diagnostic information from the digital pathology slide database. In this paper, we propose a novel pathology image retrieval method for breast cancer. It firstly applies block Local Binary Pattern (LBP) features to describe the spatial texture property of pathology image, and then use them to construct the probabilistic latent semantic analysis (pLSA) model which generally takes advantage of visual words to mine the topic-level representation of image and thus reveals the high-level semantics. Different from conventional pLSA model, we employ low-rank and sparse matrix composition for describing the correlated and specific characteristics of visual words. Therefore, the more discriminative topic-level representation corresponding to each pathology image can be obtained. Experimental results on the digital pathology image database for breast cancer demonstrate the feasibility and effectiveness of our method.
              </p>
              <pre xml:space="preserve" id="zhengIGTA2014Bib" class="bibtex" style="display: none;">
@inproceedings{zhengIGTA14,
  title     = {Pathology Image Retrieval by Block LBP Based PLSA Model
               with Low-Rank and Sparse Matrix Decomposition},
  author    = {Yushan Zheng and Zhiguo Jiang and Jun Shi and Yibing Ma},
  booktitle = {Advances in Image and Graphics Technologies},
  pages     = {327—-335},
  year      = {2014}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('zhengIGTA2014Abs');
                hideblock('zhengIGTA2014Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_shi_jei_2014.png" alt="Image" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Regularized least square discriminant projection and feature selection</b> <a href="https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-23/issue-01/013003/Regularized-least-square-discriminant-projection-and-feature-selection/10.1117/1.JEI.23.1.013003.full" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Jun Shi, Zhiguo Jiang, Danpei Zhao, Hao Feng, Chao Gao
                </i></font>
                <br>
                Journal of Electronic Imaging, 2014
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('shiJEI2014Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('shiJEI2014Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="shiJEI2014Abs" class="abstract" style="display: none;">
                  Conventional graph embedding framework uses the Euclidean distance to determine the similarities of neighbor samples, which causes the graph structure to be sensitive to outliers and lack physical interpretation. Moreover, the graph construction suffers from the difficulty of neighbor parameter selection. Although sparse representation (SR) based graph embedding methods can automatically select the neighbor parameter, the computational cost of SR is expensive. On the other hand, most discriminant projection methods fail to perform feature selection. In this paper, we present a novel joint discriminant analysis and feature selection method that employs regularized least square for graph construction and l 2; 1-norm minimization on projection matrix for feature selection. Specifically, our method first uses the regularized least square coefficients to measure the intraclass and interclass similarities from the viewpoint of reconstruction. Based on this graph structure, we formulate an object function with scatter difference criterion for learning the discriminant projections, which can avoid the small sample size problem. Simultaneously, the l2; 1-norm minimization on projection matrix is applied to gain row-sparsity for selecting useful features. Experiments on two face databases (ORL and AR) and COIL-20 object database demonstrate that our method not only achieves better classification performance, but also has lower computational cost than SR. (C) 2014 SPIE and IS&T
                </p>
              <pre xml:space="preserve" id="shiJEI2014Bib" class="bibtex" style="display: none;">
@article{shiJEI2014,
  author  = {Jun Shi and Zhiguo Jiang and Danpei Zhao and Hao Feng and Chao Gao},
  title   = {Regularized least square discriminant projection and feature selection},
  journal = {Journal of Electronic Imaging},
  volume  = {23},
  pages   = {23 - 23 - 15},
  year    = {2014},
  doi     = {10.1117/1.JEI.23.1.013003},
  URL     = {https://doi.org/10.1117/1.JEI.23.1.013003},
}             
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('shiJEI2014Abs');
                hideblock('shiJEI2014Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_shi_NPL_2014.png" alt="Image" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Adaptive Graph Embedding Discriminant Projections</b> <a href="https://link.springer.com/article/10.1007%2Fs11063-013-9323-8" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Jun Shi, Zhiguo Jiang and Hao Feng
                </i></font>
                <br>
                Neural Processing Letters, 2014
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('shiNPL2014Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('shiNPL2014Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="shiNPL2014Abs" class="abstract" style="display: none;">
                  Graph embedding based learning method plays an increasingly significant role on dimensionality reduction (DR). However, the selection to neighbor parameters of graph is intractable. In this paper, we present a novel DR method called adaptive graph embedding discriminant projections (AGEDP). Compared with most existing DR methods based on graph embedding, such as marginal Fisher analysis which usually predefines the intraclass and interclass neighbor parameters, AGEDP applies all the homogeneous samples for constructing the intrinsic graph, and simultaneously selects heterogeneous samples within the neighborhood generated by the farthest homogeneous sample for constructing the penalty graph. Therefore, AGEDP not only greatly enhances the intraclass compactness and interclass separability, but also adaptively performs neighbor parameter selection which considers the fact that local manifold structure of each sample is generally different. Experiments on AR and COIL-20 datasets demonstrate the effectiveness of the proposed method for face recognition and object categorization, and especially under the interference of occlusion, noise and poses, it is superior to other graph embedding based methods with three different classifiers: nearest neighbor classifier, sparse representation classifier and linear regression classifier.
                </p>
              <pre xml:space="preserve" id="shiNPL2014Bib" class="bibtex" style="display: none;">
@Article{ShiNPL2014,
  author  = {Jun Shi and Zhiguo Jiang and Hao Feng},
  title   = {Adaptive Graph Embedding Discriminant Projections},
  journal = {Neural Processing Letters},
  year    = {2014},
  volume  = {40},
  number  = {3},
  pages   = {211--226},
  doi     = {10.1007/s11063-013-9323-8},
}     
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('shiNPL2014Abs');
                hideblock('shiNPL2014Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
        </tbody></table>

        <hr />
        <!-- -------------------------------------------- -->
        <!-- -------------------------------------------- -->
        <h3>2013</h3>
        <table><tbody>
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/article_wu_icig_2013.jpg" alt="Flowchart" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b>Shadow boundaries identification in single natural images via multiple kernels learning</b> <a href="https://ieeexplore.ieee.org/document/6643694/" target="_blank"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Junfeng Wu,Zhiguo Jiang,Junli Yang and Jianwei Luo 
                  </i></font>
                  <br>
                  International Conference on Image and Graphics (ICIG),2013
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('wuICIG2013Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('wuICIG2013Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="wuICIG2013Abs" class="abstract" style="display: none;">
                    The identification of shadow and shading boundaries is a key step towards reducing the imaging effects that are caused by direct illumination of the light source in the scene. Discriminating shadow boundaries from images of natural scenes has been widely applied in the field of computer vision such as object recognition, intelligent monitoring and image understanding. In this paper, we propose a method to identify shadow boundaries based on multiple kernel learning. We first extract all possible candidate boundaries and then analyze their properties. Unlike the previous proposed methods which simply combine features as a vector, we choose the optimal kernel function for every feature and learn the correct weights of different features from training database. At last, we link shadow boundaries fragments together to get longer and complete shadow boundaries. The experiment results show that the method we propose works well in shadow boundaries identification.
                </p>
                <pre xml:space="preserve" id="wuICIG2013Bib" class="bibtex" style="display: none;">
  @inproceedings{wuICIG2013,
    title     = {Shadow boundaries identification in single natural images via multiple kernels learning},
    author    = {Junfeng Wu and Zhiguo Jiang and Junli Yang and Jianwei Luo },
    booktitle = {2013 Image and Graphics (ICIG)},
    doi       = {10.1109/ICIG.2013.75},
    pages     = {348-352},
    year      = {2013}
  }
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('wuICIG2013Abs');
                  hideblock('wuICIG2013Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/article_Haopeng_sci_2013.jpg" alt="Result" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b> Joint Object and Pose Recognition Using Homeomorphic Manifold Analysis</b> <a href="https://dl.acm.org/citation.cfm?id=2891601"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang, Tarek El-Gaaly, Ahmed Elgammal, Zhiguo Jiang
                  </i></font>
                  <br>
                    AAAI Conference on Artificial Intelligence, 2013
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2013Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2013Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="HaopengSCI2013Abs" class="abstract" style="display: none;">
                    Object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/ depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parametrized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we construct a manifold descriptor from this mapping between homeomorphic manifolds and use it to jointly solve the three challenging recognition sub-problems. We extensively experiment on a challenging multi-modal (i.e. RGBD) dataset and other object pose datasets and achieve state-of-the-art results.
                  </p>
                <pre xml:space="preserve" id="HaopengSCI2013Bib" class="bibtex" style="display: none;">
    @article{HaopengAAAI2013,
    title      = {Joint Object and Pose Recognition Using Homeomorphic Manifold Analysis},
    author     = {Haopeng Zhang, Tarek El-Gaaly, Ahmed Elgammal, Zhiguo Jiang},
    conference = {The 27th AAAI Conference on Artificial Intelligence},
    pages      = {1012-1019},
    year       = {2013},
    url        = {https://dl.acm.org/citation.cfm?id=2891601},
    }      
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('HaopengSCI2013Abs');
                  hideblock('HaopengSCI2013Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/article_Haopeng_sci_2013_1.jpg" alt="Result" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b> Vision-based pose estimation for cooperative space objects</b> <a href="https://www.sciencedirect.com/science/article/pii/S0094576513001781"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang, higuo Jiang, Ahmed Elgammal
                  </i></font>
                  <br>
                    Acta Astronautica, 2013
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2013_1Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2013_1Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="HaopengSCI2013_1Abs" class="abstract" style="display: none;">
                    Imaging sensors are widely used in aerospace recently. In this paper, a vision-based approach for estimating the pose of cooperative space objects is proposed. We learn generative model for each space object based on homeomorphic manifold analysis. Conceptual manifold is used to represent pose variation of captured images of the object in visual space, and nonlinear functions mapping between conceptual manifold representation and visual inputs are learned. Given such learned model, we estimate the pose of a new image by minimizing a reconstruction error via a traversal procedure along the conceptual manifold. Experimental results on the simulated image dataset show that our approach is effective for 1D and 2D pose estimation.
                  </p>
                <pre xml:space="preserve" id="HaopengSCI2013_1Bib" class="bibtex" style="display: none;">
    @article{HaopengAA2013,
    title      = {Vision-based pose estimation for cooperative space objects},
    author     = {Haopeng Zhang, higuo Jiang, Ahmed Elgammal},
    Journal    = {Acta Astronautica},
    pages      = {115-122},
    year       = {2013},
    url        = {https://www.sciencedirect.com/science/article/pii/S0094576513001781},
    }      
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('HaopengSCI2013_1Abs');
                  hideblock('HaopengSCI2013_1Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Feng_JEI_2013.png" alt="Image" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Unsupervised texture segmentation based on latent topic assignment</b> <a href="https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-22/issue-01/013026/Unsupervised-texture-segmentation-based-on-latent-topic-assignment/10.1117/1.JEI.22.1.013026.full?SSO=1" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Hao Feng, Zhiguo Jiang and Jun Shi
                </i></font>
                <br>
                Journal of Electronic Imaging, 2013
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('fengJEI2013Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('fengJEI2013Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="fengJEI2013Abs" class="abstract" style="display: none;">
                We present an effective solution for unsupervised texture segmentation by taking advantage of the latent Dirichlet allocation (LDA) model. LDA is a generative topic model that is capable of hierarchically organizing discrete data including texts and images. We propose a new texture model by connecting texture primitives to the topic of LDA. The model is able to extract the characteristic features of a texture primitive and group them into a topic based on their frequencies of co-occurrence. Here, the feature descriptor is the connection of Haar-like features of multiple sizes. The segments of an image are finally obtained by identifying the homogeneous regions in the corresponding topic assignment map. The evaluation results for synthetic texture mosaics, remote sensing images, and natural scene images are illustrated.
              </p>
              <pre xml:space="preserve" id="fengJEI2013Bib" class="bibtex" style="display: none;">
@article{fengJEI2013,
  author  = {Hao  Feng and Zhiguo Jiang and Jun  Shi},
  title   = {Unsupervised texture segmentation based on latent topic assignment},
  journal = {Journal of Electronic Imaging},
  volume  = {22},
  number  = {1},
  pages   = {13-26},
  year    = {2013},
  doi     = {10.1117/1.JEI.22.1.013026},
  URL     = {https://doi.org/10.1117/1.JEI.22.1.013026},
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('fengJEI2013Abs');
                hideblock('fengJEI2013Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_shi_ICIG_2013.jpg" alt="Image" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Pathological Image Retrieval for Breast Cancer with pLSA Model</b> <a href="http://ieeexplore.ieee.org/document/6643748/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Jun Shi, Yibing Ma, Zhiguo Jiang, Hao Feng, Jin Chen and Yu Zhao
                </i></font>
                <br>
                International Conference on Image and Graphics (ICIG), 2013
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('shiICIG2013Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('shiICIG2013Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="shiICIG2013Abs" class="abstract" style="display: none;">
                Pathological image retrieval contributes to computer-aided diagnosis for breast cancer due to the fact that the retrieval results generally contain detailed diagnostic information (e.g. abnormal regions and diagnostic opinion from other doctors) which can offer some reference and assistance to the doctor during diagnosis process. In this paper, we present a novel pathological image retrieval approach based on probabilistic latent semantic analysis (pLSA) model. The method respectively utilizes SIFT features after visual saliency detection, and block Gabor features for the construction of two semantic codebooks, which not only can characterize the salient local invariant features and texture information under different scales and orientations in the pathological images, but also consider the high-level semantic features. Furthermore, we apply pLSA model to discover the latent topics in each codebook. Finally each pathological image is represented by the combination of topics from these two codebooks. The proposed method is evaluated on the pathological image database for breast cancer, which includes 5 categories (mucinous cystadenocarcinoma, invasive lobular carcinoma, basal-like carcinoma, invasive breast cancer and low-grade adenosquamous carcinoma) and 110 cases for each category. Experimental results demonstrate the feasibility and effectiveness of our method.
              </p>
              <pre xml:space="preserve" id="shiICIG2013Bib" class="bibtex" style="display: none;">
@inproceedings{shiICIG2013, 
  author    = {Jun Shi and Yibing Ma and Zhiguo Jiang and Hao Feng and Jin Chen and Yu Zhao}, 
  booktitle = {2013 Seventh International Conference on Image and Graphics}, 
  title     = {Pathological Image Retrieval for Breast Cancer with pLSA Model}, 
  year      = {2013}, 
  pages     = {634-638}, 
  doi       = {10.1109/ICIG.2013.131},
  month     = {July}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('shiICIG2013Abs');
                hideblock('shiICIG2013Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Ma_ICIG_2013.JPG" alt="Image" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>PLSA-based pathological image retrieval for breast cancer with color deconvolution</b> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8920/1/PLSA-based-pathological-image-retrieval-for-breast-cancer-with-color/10.1117/12.2032054.full" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Yibing Ma, Jun Shi, Zhiguo Jiang and Hao Feng
                </i></font>
                <br>
                International Symposium on Multispectral Image Processing and Pattern Recognition (MIPPR), 2013
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('maMIPPR2013Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('maMIPPR2013Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="maMIPPR2013Abs" class="abstract" style="display: none;">
                Digital pathological image retrieval plays an important role in computer-aided diagnosis for breast cancer. The retrieval results of an unknown pathological image, which are generally previous cases with diagnostic information, can provide doctors with assistance and reference. In this paper, we develop a novel pathological image retrieval method for breast cancer, which is based on stain component and probabilistic latent semantic analysis (pLSA) model. Specifically, the method firstly utilizes color deconvolution to gain the representation of different stain components for cell nuclei and cytoplasm, and then block Gabor features are conducted on cell nuclei, which is used to construct the codebook. Furthermore, the connection between the words of the codebook and the latent topics among images are modeled by pLSA. Therefore, each image can be represented by the topics and also the high-level semantic concepts of image can be described. Experiments on the pathological image database for breast cancer demonstrate the effectiveness of our method.
              </p>
              <pre xml:space="preserve" id="maMIPPR2013Bib" class="bibtex" style="display: none;">
@proceeding{maMIPPR2013,
  author  = {Yibing Ma and Jun Shi and Zhiguo Jiang and Hao Feng},
  title   = {PLSA-based pathological image retrieval for breast cancer with color deconvolution},
  journal = {Proc.SPIE},
  volume  = {8920},
  pages   = {8920-7},
  year    = {2013},
  doi     = {10.1117/12.2032054},
  URL     = {https://doi.org/10.1117/12.2032054},
}          
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('maMIPPR2013Abs');
                hideblock('maMIPPR2013Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/airticle_Shi_igarss_2013.jpg" alt="Image" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>Sparse coding-based topic model for remote sensing image segmentation</b> <a href="http://ieeexplore.ieee.org/document/6723740/" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Jun Shi, Zhiguo Jiang, Hao Feng and Yibing Ma
                </i></font>
                <br>
                IEEE International Conference on Geoscience and Remote Sensing Symposium (IGARSS), 2013
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('shiIGARSS2013Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('shiIGARSS2013Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="shiIGARSS2013Abs" class="abstract" style="display: none;">
                Land cover segmentation can be viewed as topic assignment that the pixels are grouped into homogeneous regions according to different semantic topics in topic model. In this paper, we propose a novel topic model based on sparse coding for segmenting different kinds of land covers. Different from conventional topic models which generally assume each local feature descriptor is related to only one visual word of the codebook, our method utilizes sparse coding to characterize the potential correlation between the descriptor and multiple words. Therefore each descriptor can be represented by a small set of words. Furthermore, in this paper probabilistic Latent Semantic Analysis (pLSA) is applied to learn the latent relation among word, topic and document due to its simplicity and low computational cost. Experimental results on remote sensing image segmentation demonstrate the excellent superiority of our method over k-means clustering and conventional pLSA model.
              </p>
              <pre xml:space="preserve" id="shiIGARSS2013Bib" class="bibtex" style="display: none;">
@inproceedings{shiIGARSS2013, 
  author    = {Jun Shi and Zhiguo Jiang and Hao Feng and Yibing Ma}, 
  booktitle = {2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS}, 
  title     = {Sparse coding-based topic model for remote sensing image segmentation}, 
  year      = {2013}, 
  pages     = {4122-4125}, 
  doi       = {10.1109/IGARSS.2013.6723740}, 
  ISSN      = {2153-6996}, 
  month     = {July}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('shiIGARSS2013Abs');
                hideblock('shiIGARSS2013Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
        </tbody></table>

        <hr />
        <h3>Before 2013</h3>
        <table><tbody>
            <tr> <!-- An Paper -->
              <td width="22%" valign="top"><p>
                <img src="images/src/article_Haopeng_sci_2012.jpg" alt="Result" width="200">
              </p></td>
              <td width="78%" valign="top">
                <p>
                  <b> Space object, high-resolution, optical imaging simulation of space-based systems</b> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8385/1/Space-object-high-resolution-optical-imaging-simulation-of-space-based/10.1117/12.918368.full"><i class="fa fa-external-link"></i></a>
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Haopeng Zhang; Wei Zhang; Zhiguo Jiang
                  </i></font>
                  <br>
                    SPIE Defense, Security, and Sensing, 2012
                  <br>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2012Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2012Bib')">BibTeX</a> &nbsp;
                </p>
                <p id="HaopengSCI2012Abs" class="abstract" style="display: none;">
                    Acquiring optical images of space objects is one of the most important goals of space-based optical surveillance systems. However, it's actually difficult to obtain enough high resolution optical images for space object recognition, attitude measurement and situational awareness. To solve this problem, the imaging model of space-based optical camera and the imaging characteristics of space objects are analyzed in this paper, and a novel method of image simulation is proposed. The high resolution images of space objects simulated by our method are visually similar to the actual imaging results and may provide data support for further research on space technology.
                  </p>
                <pre xml:space="preserve" id="HaopengSCI2012Bib" class="bibtex" style="display: none;">
    @article{HaopengSPIE2013,
    title      = {Space object, high-resolution, optical imaging simulation of space-based systems},
    author     = {Haopeng Zhang; Wei Zhang; Zhiguo Jiang},
    booktitle  = {SPIE Defense, Security, and Sensing},
    pages      = {7},
    year       = {2012},
    url        = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8385/1/Space-object-high-resolution-optical-imaging-simulation-of-space-based/10.1117/12.918368.full},
    }      
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('HaopengSCI2012Abs');
                  hideblock('HaopengSCI2012Bib');
                </script>
              </td>
            </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/airticle_Shi_icip_2012.jpg" alt="Image" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b>SIFT-based Elastic sparse coding for image retrieval</b> <a href="http://ieeexplore.ieee.org/document/6467390" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                  Jun Shi, Zhiguo Jiang, Hao Feng and Liguo Zhang
                </i></font>
                <br>
                IEEE International Conference on Image Processing (ICIP), 2012
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('shiICIP2012Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('shiICIP2012Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="shiICIP2012Abs" class="abstract" style="display: none;">
                Bag-of-features (BoF) model based on SIFT generally assumes each descriptor is related to only one visual word of the codebook. Therefore, the potential correlation between the descriptor and other visual words is ignored. On the other hand, sparse coding through l1-norm regularization fails to generate optimal sparse representations since l1-norm regularization randomly selected one variable from a group of highly correlated variables. In this study we propose a novel bag-of-features model for image retrieval called SIFT-based Elastic sparse coding. The method utilizes a large number of SIFT descriptors to construct the codebook. The Elastic Net regression framework, which combines both l1-norm and l2-norm penalties, is then used to obtain the sparse-coefficient vector corresponding to the SIFT descriptor. Finally each image can be represented by a unified sparse-coefficient vector. Experimental results on Coil20 dataset demonstrate the consistent superiority of the proposed method over the state-of-the-art algorithms including original SIFT matching, conventional BoF strategy and BoF model based on l1-norm sparse coding.
              </p>
              <pre xml:space="preserve" id="shiICIP2012Bib" class="bibtex" style="display: none;">
@inproceedings{shi2012ICIP, 
  author    = {Jun Shi and Zhiguo Jiang and Hao Feng and Liguo Zhang}, 
  booktitle = {2012 19th IEEE International Conference on Image Processing}, 
  title     = {SIFT-based Elastic sparse coding for image retrieval}, 
  year      = {2012},
  pages     = {2437-2440}, 
  doi       = {10.1109/ICIP.2012.6467390}, 
  ISSN      = {1522-4880}, 
  month     = {Sep}
}
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('shiICIP2012Abs');
                hideblock('shiICIP2012Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Haopeng_sci_2011.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b> Image segmentation based on pixel feature manifold</b> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8003/1/Image-segmentation-based-on-pixel-feature-manifold/10.1117/12.902145.full"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Haopeng Zhang; Zhiguo Jiang
                </i></font>
                <br>
                International Symposium on Multispectral Image Processing and Pattern Recognition (MIPPR), 2011
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('HaopengSCI2011Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('HaopengSCI2011Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="HaopengSCI2011Abs" class="abstract" style="display: none;">
                  Image segmentation is an important problem in pattern recognition, computer vision and other related area, which is still a research focus. In this paper, we consider the segmentation as pixel classification scheme and introduce a manifold way to address this problem. Some local features, such as Haar, LBP and SIFT, are used to represent each pixel in the image together with the basic property of the pixel. We put these pixel features on a manifold called pixel feature manifold (PFM) obtained via manifold learning methods and classify pixels with k-NN classifier in the pixel embedding space. Experimental results on MSRC image dataset show that our PFM method can effectively segment images. 
                </p>
              <pre xml:space="preserve" id="HaopengSCI2011Bib" class="bibtex" style="display: none;">
@inproceedings{Zhang2011Image,
  title={Image Segmentation Based on Pixel Feature Manifold},
  author={Zhang, Haopeng and Jiang, Zhiguo and Zhang, Wei and Zhao, Danpei},
  booktitle={SPIE MIPPR},
  pages={800307},
  year={2011},
}    
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('HaopengSCI2011Abs');
                hideblock('HaopengSCI2011Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Meng_2010.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b> Full-viewpoint 3D space object recognition based on kernel locality preserving projections</b> <a href="https://www.sciencedirect.com/science/article/pii/S1000936109602557"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Gang Meng, Zhiguo Jiang, Zhengyi Liu, Haopeng Zhang and Danpei Zhao
                </i></font>
                <br>
               Chinese Journal of Aeronautics, 2010
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('Meng_2010Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('Meng_2010Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="Meng_2010Abs" class="abstract" style="display: none;">
                  Space object recognition plays an important role in spatial exploitation and surveillance,followed by two main problems:lacking of data and drastic changes in viewpoints.In this article,firstly,we build a three-dimensional (3D) satellites dataset named BUAA Satellite Image Dataset (BUAA-SID 1.0) to supply data for 3D space object research.Then,based on the dataset,we propose to recognize full-viewpoint 3D space objects based on kemel locality preserving projections (KLPP).To obtain more accurate and separable description of the objects,firstly,we build feature vectors employing moment invariants,Fourier descriptors,region covariance and histogram of oriented gradients.Then,we map the features into kernel space followed by dimensionality reduction using KLPP to obtain the submanifold of the features.At last,k-nearest neighbor (kNN) is used to accomplish the classification.Experimental results show that the proposed approach is more appropriate for space object recognition mainly considering changes of viewpoints.Encouraging recognition rate could be obtained based on images in BUAA-SID 1.0,and the highest recognition result could achieve 95.87%.
                </p>
              <pre xml:space="preserve" id="Meng_2010Bib" class="bibtex" style="display: none;">
@article{Meng2010CJA,
  title = {Full-viewpoint 3D Space Object Recognition Based on Kernel Locality Preserving Projections},
  author = {Meng, Gang and Jiang, Zhiguo and Liu, Zhengyi and Zhang, Haopeng and Zhao, Danpei},
  journal = {Chinese Journal of Aeronautics},
  volume = {23},
  number = {5},
  pages = {563 - 572},
  year = {2010},
  doi = {doi.org/10.1016/S1000-9361(09)60255-7},
}    
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('Meng_2010Abs');
                hideblock('Meng_2010Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Meng_2009.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b> The usage of color invariance in SURF</b> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7495/1/The-usage-of-color-invariance-in-SURF/10.1117/12.833509.full"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Gang Meng; Zhiguo Jiang; Danpei Zhao
                </i></font>
                <br>
                International Symposium on Multispectral Image Processing and Pattern Recognition (MIPPR), 2009
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('Meng_2009Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('Meng_2009Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="Meng_2009Abs" class="abstract" style="display: none;">
                  Space object recognition plays an important role in spatial exploitation and surveillance,followed by two main problems:lacking of data and drastic changes in viewpoints.In this article,firstly,we build a three-dimensional (3D) satellites dataset named BUAA Satellite Image Dataset (BUAA-SID 1.0) to supply data for 3D space object research.Then,based on the dataset,we propose to recognize full-viewpoint 3D space objects based on kemel locality preserving projections (KLPP).To obtain more accurate and separable description of the objects,firstly,we build feature vectors employing moment invariants,Fourier descriptors,region covariance and histogram of oriented gradients.Then,we map the features into kernel space followed by dimensionality reduction using KLPP to obtain the submanifold of the features.At last,k-nearest neighbor (kNN) is used to accomplish the classification.Experimental results show that the proposed approach is more appropriate for space object recognition mainly considering changes of viewpoints.Encouraging recognition rate could be obtained based on images in BUAA-SID 1.0,and the highest recognition result could achieve 95.87%.
                </p>
              <pre xml:space="preserve" id="Meng_2009Bib" class="bibtex" style="display: none;">
@inproceedings{MengMIPPR2009,
  title      = {The usage of color invariance in SURF},
  author     = {Gang Meng; Zhiguo Jiang; Danpei Zhao},
  booktitle  = {Sixth International Symposium on Multispectral Image Processing and Pattern Recognition},
  pages      = {7},
  year       = {2009},
  doi        = {10.1117/12.833509},
}      
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('Meng_2009Abs');
                hideblock('Meng_2009Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Meng_2009_1.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b> Maneuvering Target Tracking in Cluttered Background Based on Color Invariance and Support Vector Machine</b> <a href="https://ieeexplore.ieee.org/document/5437928/"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Gang Meng ;  Zhiguo Jiang ;  Danpei Zhao ;  Yue Gao
                </i></font>
                <br>
                International Conference on Image and Graphics (ICIG), 2009
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('Meng_2009_1Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('Meng_2009_1Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="Meng_2009_1Abs" class="abstract" style="display: none;">
                  Maneuvering targets tracking in cluttered environment is a challenging problem in computer vision because of the difficulty of distinguishing the target from the background. In this paper, we treat tracking as a binary classification problem and employ support vector machine to suppress the background. In order to enhance the robustness against illumination changes, we propose to combine color invariance with traditional RGB values to train the SVM. First, we use expectation maximization algorithm to extract the target from the environment; then, RGB and color invariance values are used to train SVM. In the incoming frames, pixels in regions of interest are classified by SVM and the confidence map is produced, which will afterward be used by traditional tracking approach to track the target, in this paper, we employ particle filter. Experimental results on challenging sequences validate the effectiveness of the proposed method in cluttered background target tracking.
                </p>
              <pre xml:space="preserve" id="Meng_2009_1Bib" class="bibtex" style="display: none;">
  @inproceedings{MengIG2009,
  title      = {Maneuvering Target Tracking in Cluttered Background Based on Color Invariance and Support Vector Machine},
  author     = {Gang Meng ;  Zhiguo Jiang ;  Danpei Zhao ;  Yue Gao},
  booktitle  = {International Conference on Image and Graphics},
  pages      = {510-515},
  year       = {2009},
  doi        = {10.1109/ICIG.2009.155},
  url        = {https://ieeexplore.ieee.org/document/5437928/figures},
  }      
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('Meng_2009_1Abs');
                hideblock('Meng_2009_1Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->
          <tr> <!-- An Paper -->
            <td width="22%" valign="top"><p>
              <img src="images/src/article_Meng_2009_2.jpg" alt="Result" width="200">
            </p></td>
            <td width="78%" valign="top">
              <p>
                <b> Real-time illumination robust maneuvering target tracking based on color invariance</b> <a href="https://ieeexplore.ieee.org/document/5301491/"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Gang Meng; Zhiguo Jiang; Danpei Zhao; Keren Ye
                </i></font>
                <br>
                Image and Signal Processing, 2009
                <br>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('Meng_2009_2Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('Meng_2009_2Bib')">BibTeX</a> &nbsp;
              </p>
              <p id="Meng_2009_2Abs" class="abstract" style="display: none;">
                  Illumination change is an important factor that affects the accuracy of tracking. In order to enhance the robustness against illumination changes in directions and intensities of the conventional tracking approaches, this paper proposes to apply color invariance theory to maneuvering target tracking. To depress the influence of the background, at the beginning of the process, we use expectation maximization algorithm based on Gaussian mixture model to extract the objects from the environment. Then, instead of using the gray space or RGB space, frames are transformed to color invariant space based on Kubelka-Munk model, followed by the implementation of conventional tracking approach such as mean shift or particle filtering to track the targets. Experimental results show that mean shift and improved particle filtering with color invariance can achieve 64.1 and 16.0 frames per second correspondingly, which is real-time, and the proposed approach improves the robustness of maneuvering targets tracking greatly.
                </p>
              <pre xml:space="preserve" id="Meng_2009_2Bib" class="bibtex" style="display: none;">
  @article{MengISP2009,
  title      = {Real-time illumination robust maneuvering target tracking based on color invariance},
  author     = {Gang Meng; Zhiguo Jiang; Danpei Zhao; Keren Ye},
  journal    = {Image and Signal Processing},
  pages      = {1-5},
  year       = {2009},
  doi        = { 10.1109/CISP.2009.5301491},
  url        = {https://ieeexplore.ieee.org/document/5301491/},
  }      
              </pre>
              <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('Meng_2009_2Abs');
                hideblock('Meng_2009_2Bib');
              </script>
            </td>
          </tr> <!-- Paper End Here -->      
        </tbody></table>
      </div>
      </div>
      </article>
      </div>
      </div>
    </section>
    </div><!-- /.container -->

    <footer class="footer">
      <div class="footer-bottom">
        <i class="fa fa-copyright"></i> Copyright 2018. All rights reserved.<br>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
      ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="scripts/jquery.min.js"></script>
    <script src="scripts/bootstrap.min.js"></script>
    <script src="scripts/jquery.bxslider.js"></script>
    <script src="scripts/mooz.scripts.min.js"></script>
    <script src="scripts/togglehide.js"></script>
  </body>
</html>
